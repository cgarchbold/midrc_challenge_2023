{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from midrc_dataset import midrc_SIMCLR_dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from get_model import create_model\n",
    "import torchvision\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cross_fold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLR_Loss(nn.Module):\n",
    "    def __init__(self, batch_size, temperature):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "\n",
    "        #self.mask = self.mask_correlated_samples(batch_size)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        self.similarity_f = nn.CosineSimilarity(dim=2)\n",
    "\n",
    "    def mask_correlated_samples(self, batch_size):\n",
    "        N = 2 * batch_size\n",
    "        mask = torch.ones((N, N), dtype=bool)\n",
    "        mask = mask.fill_diagonal_(0)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            mask[i, batch_size + i] = 0\n",
    "            mask[batch_size + i, i] = 0\n",
    "        return mask\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "\n",
    "        mask = self.mask_correlated_samples(z_i.shape[0])\n",
    "\n",
    "        N = 2 * z_i.shape[0]\n",
    "\n",
    "        z = torch.cat((z_i, z_j), dim=0)\n",
    "\n",
    "        sim = self.similarity_f(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n",
    "\n",
    "        sim_i_j = torch.diag(sim, z_i.shape[0])\n",
    "        sim_j_i = torch.diag(sim, -z_i.shape[0])\n",
    "        \n",
    "        # We have 2N samples, but with Distributed training every GPU gets N examples too, resulting in: 2xNxN\n",
    "        #print(sim_i_j.shape, sim_j_i.shape)\n",
    "        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n",
    "        #print(\"Positive Done\")\n",
    "        negative_samples = sim[mask].reshape(N, -1)\n",
    "        #print(\"Negative Done\")\n",
    "        \n",
    "        #SIMCLR\n",
    "        labels = torch.from_numpy(np.array([0]*N)).reshape(-1).to(positive_samples.device).long() #.float()\n",
    "        \n",
    "        logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss /= N\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs,model,device, train_loader, val_loader, criterion, optimizer, experiment_name, fold_number):\n",
    "    metrics={}\n",
    "    best_vloss = 1_000_000.\n",
    "    for e in range(epochs):\n",
    "        print('EPOCH {}:'.format(e + 1))\n",
    "        running_loss = 0.\n",
    "        last_loss = 0.\n",
    "        running_train_kappa=0.0\n",
    "        avg_train_kappa=0.0\n",
    "        model.train(True)\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            # Every data instance is an input + label pair\n",
    "            img1, img2, score1, score2 = data\n",
    "\n",
    "            #print(\"img1 and img2 shape\", img1.shape, img2.shape)\n",
    "\n",
    "            # Zero your gradients for every batch!\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Make predictions for this batch\n",
    "            output1 = model(img1.to(device))\n",
    "            output2 = model(img2.to(device))\n",
    "\n",
    "            # Compute the loss and its gradients\n",
    "            loss = criterion(output1, output2) \n",
    "            loss.backward()\n",
    "\n",
    "            # Adjust learning weights\n",
    "            optimizer.step()\n",
    "        \n",
    "            # Gather data\n",
    "            running_loss += loss\n",
    "        \n",
    "        avg_loss = running_loss/(i+1)\n",
    "        \n",
    "        running_vloss = 0.0\n",
    "        running_kappa = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(val_loader):\n",
    "                img1, img2, score1, score2 = vdata\n",
    "                voutput1 = model(img1.to(device))\n",
    "                voutput2 = model(img2.to(device))\n",
    "                vloss = criterion(output1,output2) # TODO: Create cosine sim loss\n",
    "                running_vloss += vloss\n",
    "\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        \n",
    "        print('LOSS train {} valid {}'.format(avg_loss,avg_vloss))\n",
    "        metrics[e] = {      \n",
    "            f'Fold {fold_number} avg_train_loss': avg_loss.item(),\n",
    "            f'Fold {fold_number} avg_val_loss': avg_vloss.item(),\n",
    "            f'Fold {fold_number} epoch': e\n",
    "        }\n",
    "        #if config['wandb']==True:\n",
    "        #    wandb.log(metrics[e])\n",
    "\n",
    "        # Track best performance, and save the model's state\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = os.path.join('experiments',experiment_name,'saved_models', 'modelsave_fold_{}.ckpt'.format(fold_number))\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1  Train Indices:  2078  Val indices:  520\n",
      "Fold:  2  Train Indices:  2096  Val indices:  502\n",
      "Fold:  3  Train Indices:  2050  Val indices:  548\n",
      "Fold:  4  Train Indices:  2072  Val indices:  526\n",
      "Fold:  5  Train Indices:  2096  Val indices:  502\n",
      "FOLD:  1\n",
      "172\n",
      "44\n",
      "EPOCH 1:\n",
      "LOSS train 1.8970226049423218 valid 0.22975686192512512\n",
      "EPOCH 2:\n",
      "LOSS train 1.506740689277649 valid 0.24054855108261108\n",
      "EPOCH 3:\n",
      "LOSS train 1.4740350246429443 valid 0.32486462593078613\n",
      "EPOCH 4:\n",
      "LOSS train 1.5043015480041504 valid 0.5651783347129822\n",
      "EPOCH 5:\n",
      "LOSS train 1.4851230382919312 valid 0.24748072028160095\n",
      "EPOCH 6:\n",
      "LOSS train 1.5221606492996216 valid 0.3497157096862793\n",
      "EPOCH 7:\n",
      "LOSS train 1.464235544204712 valid 0.23155587911605835\n",
      "EPOCH 8:\n",
      "LOSS train 1.4599196910858154 valid 0.31904396414756775\n",
      "EPOCH 9:\n",
      "LOSS train 1.4409003257751465 valid 0.227624773979187\n",
      "EPOCH 10:\n",
      "LOSS train 1.4630277156829834 valid 0.30672022700309753\n",
      "EPOCH 11:\n",
      "LOSS train 1.4600430727005005 valid 0.26307082176208496\n",
      "EPOCH 12:\n",
      "LOSS train 1.452674388885498 valid 0.21602025628089905\n",
      "EPOCH 13:\n",
      "LOSS train 1.4108582735061646 valid 0.22727124392986298\n",
      "EPOCH 14:\n",
      "LOSS train 1.4437757730484009 valid 0.27006077766418457\n",
      "EPOCH 15:\n",
      "LOSS train 1.4023805856704712 valid 0.21804051101207733\n",
      "EPOCH 16:\n",
      "LOSS train 1.4088324308395386 valid 0.24733798205852509\n",
      "EPOCH 17:\n",
      "LOSS train 1.3879132270812988 valid 0.22412702441215515\n",
      "EPOCH 18:\n",
      "LOSS train 1.4014630317687988 valid 0.22669720649719238\n",
      "EPOCH 19:\n",
      "LOSS train 1.3793781995773315 valid 0.20420388877391815\n",
      "EPOCH 20:\n",
      "LOSS train 1.3875714540481567 valid 0.22729462385177612\n",
      "EPOCH 21:\n",
      "LOSS train 1.412516474723816 valid 0.20332428812980652\n",
      "EPOCH 22:\n",
      "LOSS train 1.393357753753662 valid 0.2260931432247162\n",
      "EPOCH 23:\n",
      "LOSS train 1.3966505527496338 valid 0.2283296287059784\n",
      "EPOCH 24:\n",
      "LOSS train 1.3771462440490723 valid 0.24173490703105927\n",
      "EPOCH 25:\n",
      "LOSS train 1.3836770057678223 valid 0.22603332996368408\n",
      "EPOCH 26:\n",
      "LOSS train 1.3722084760665894 valid 0.21231591701507568\n",
      "EPOCH 27:\n",
      "LOSS train 1.3793957233428955 valid 0.22360330820083618\n",
      "EPOCH 28:\n",
      "LOSS train 1.3817847967147827 valid 0.23933899402618408\n",
      "EPOCH 29:\n",
      "LOSS train 1.3753206729888916 valid 0.22312337160110474\n",
      "EPOCH 30:\n",
      "LOSS train 1.3737982511520386 valid 0.20706109702587128\n",
      "EPOCH 31:\n",
      "LOSS train 1.3540558815002441 valid 0.2213563621044159\n",
      "EPOCH 32:\n",
      "LOSS train 1.3901413679122925 valid 0.23177573084831238\n",
      "EPOCH 33:\n",
      "LOSS train 1.3870230913162231 valid 0.22355520725250244\n",
      "EPOCH 34:\n",
      "LOSS train 1.4032750129699707 valid 0.24697157740592957\n",
      "EPOCH 35:\n",
      "LOSS train 1.3799068927764893 valid 0.21195439994335175\n",
      "EPOCH 36:\n",
      "LOSS train 1.376915454864502 valid 0.20150330662727356\n",
      "EPOCH 37:\n",
      "LOSS train 1.3792237043380737 valid 0.2222212553024292\n",
      "EPOCH 38:\n",
      "LOSS train 1.3587459325790405 valid 0.20726776123046875\n",
      "EPOCH 39:\n",
      "LOSS train 1.3713468313217163 valid 0.2130192369222641\n",
      "EPOCH 40:\n",
      "LOSS train 1.358892560005188 valid 0.21458089351654053\n",
      "EPOCH 41:\n",
      "LOSS train 1.3636335134506226 valid 0.19119279086589813\n",
      "EPOCH 42:\n",
      "LOSS train 1.3908374309539795 valid 0.2919189929962158\n",
      "EPOCH 43:\n",
      "LOSS train 1.3727589845657349 valid 0.21990764141082764\n",
      "EPOCH 44:\n",
      "LOSS train 1.3684781789779663 valid 0.21957656741142273\n",
      "EPOCH 45:\n",
      "LOSS train 1.3801276683807373 valid 0.21694380044937134\n",
      "EPOCH 46:\n",
      "LOSS train 1.370752215385437 valid 0.20430591702461243\n",
      "EPOCH 47:\n",
      "LOSS train 1.3755433559417725 valid 0.23543815314769745\n",
      "EPOCH 48:\n",
      "LOSS train 1.3690026998519897 valid 0.20886032283306122\n",
      "EPOCH 49:\n",
      "LOSS train 1.3785442113876343 valid 0.2077370285987854\n",
      "EPOCH 50:\n",
      "LOSS train 1.3745149374008179 valid 0.20828568935394287\n",
      "FOLD:  2\n",
      "177\n",
      "39\n",
      "EPOCH 1:\n",
      "LOSS train 1.9727951288223267 valid 1.1507484912872314\n",
      "EPOCH 2:\n",
      "LOSS train 1.6244410276412964 valid 1.225768804550171\n",
      "EPOCH 3:\n",
      "LOSS train 1.5489150285720825 valid 1.1790447235107422\n",
      "EPOCH 4:\n",
      "LOSS train 1.5084611177444458 valid 1.0167653560638428\n",
      "EPOCH 5:\n",
      "LOSS train 1.5021945238113403 valid 0.9655780792236328\n",
      "EPOCH 6:\n",
      "LOSS train 1.4867831468582153 valid 1.0165135860443115\n",
      "EPOCH 7:\n",
      "LOSS train 1.477901816368103 valid 1.0898680686950684\n",
      "EPOCH 8:\n",
      "LOSS train 1.443134069442749 valid 0.981796383857727\n",
      "EPOCH 9:\n",
      "LOSS train 1.4472124576568604 valid 1.0472743511199951\n",
      "EPOCH 10:\n",
      "LOSS train 1.443274974822998 valid 0.9681090116500854\n",
      "EPOCH 11:\n",
      "LOSS train 1.4361603260040283 valid 1.0274138450622559\n",
      "EPOCH 12:\n",
      "LOSS train 1.4510862827301025 valid 1.0104351043701172\n",
      "EPOCH 13:\n",
      "LOSS train 1.4354326725006104 valid 1.0823802947998047\n",
      "EPOCH 14:\n",
      "LOSS train 1.4152206182479858 valid 0.9368392825126648\n",
      "EPOCH 15:\n",
      "LOSS train 1.4158016443252563 valid 0.9837225675582886\n",
      "EPOCH 16:\n",
      "LOSS train 1.447426199913025 valid 0.9876754283905029\n",
      "EPOCH 17:\n",
      "LOSS train 1.4500043392181396 valid 1.0111668109893799\n",
      "EPOCH 18:\n",
      "LOSS train 1.4354798793792725 valid 0.9851049184799194\n",
      "EPOCH 19:\n",
      "LOSS train 1.4944709539413452 valid 1.0345838069915771\n",
      "EPOCH 20:\n",
      "LOSS train 1.4706695079803467 valid 1.0144412517547607\n",
      "EPOCH 21:\n",
      "LOSS train 1.5278513431549072 valid 1.1296820640563965\n",
      "EPOCH 22:\n",
      "LOSS train 1.5482107400894165 valid 1.1691663265228271\n",
      "EPOCH 23:\n",
      "LOSS train 1.5120304822921753 valid 0.9903356432914734\n",
      "EPOCH 24:\n",
      "LOSS train 1.5253785848617554 valid 1.039306402206421\n",
      "EPOCH 25:\n",
      "LOSS train 1.535583257675171 valid 1.1617645025253296\n",
      "EPOCH 26:\n",
      "LOSS train 1.5156241655349731 valid 1.2533838748931885\n",
      "EPOCH 27:\n",
      "LOSS train 1.5032683610916138 valid 1.0640015602111816\n",
      "EPOCH 28:\n",
      "LOSS train 1.5266516208648682 valid 1.0213968753814697\n",
      "EPOCH 29:\n",
      "LOSS train 1.4785902500152588 valid 0.9324616193771362\n",
      "EPOCH 30:\n",
      "LOSS train 1.527830958366394 valid 1.0195735692977905\n",
      "EPOCH 31:\n",
      "LOSS train 1.5631296634674072 valid 1.0778496265411377\n",
      "EPOCH 32:\n",
      "LOSS train 1.517731785774231 valid 1.059807538986206\n",
      "EPOCH 33:\n",
      "LOSS train 1.5096079111099243 valid 1.0498888492584229\n",
      "EPOCH 34:\n",
      "LOSS train 1.4842535257339478 valid 0.9648692607879639\n",
      "EPOCH 35:\n",
      "LOSS train 1.5114272832870483 valid 1.0389845371246338\n",
      "EPOCH 36:\n",
      "LOSS train 1.4761565923690796 valid 0.9772248268127441\n",
      "EPOCH 37:\n",
      "LOSS train 1.4965413808822632 valid 1.0266845226287842\n",
      "EPOCH 38:\n",
      "LOSS train 1.4894646406173706 valid 0.978860080242157\n",
      "EPOCH 39:\n",
      "LOSS train 1.4635593891143799 valid 1.0554430484771729\n",
      "EPOCH 40:\n",
      "LOSS train 1.4487709999084473 valid 0.9748553037643433\n",
      "EPOCH 41:\n",
      "LOSS train 1.4725024700164795 valid 1.0144424438476562\n",
      "EPOCH 42:\n",
      "LOSS train 1.4728704690933228 valid 1.0612854957580566\n",
      "EPOCH 43:\n",
      "LOSS train 1.4548728466033936 valid 0.9924630522727966\n",
      "EPOCH 44:\n",
      "LOSS train 1.4614745378494263 valid 1.153123378753662\n",
      "EPOCH 45:\n",
      "LOSS train 1.4915711879730225 valid 1.0823378562927246\n",
      "EPOCH 46:\n",
      "LOSS train 1.4806504249572754 valid 1.1221593618392944\n",
      "EPOCH 47:\n",
      "LOSS train 1.4577467441558838 valid 0.9323667287826538\n",
      "EPOCH 48:\n",
      "LOSS train 1.4705013036727905 valid 0.9676990509033203\n",
      "EPOCH 49:\n",
      "LOSS train 1.4633680582046509 valid 0.9298705458641052\n",
      "EPOCH 50:\n",
      "LOSS train 1.4906058311462402 valid 1.1573090553283691\n",
      "FOLD:  3\n",
      "175\n",
      "41\n",
      "EPOCH 1:\n",
      "LOSS train 1.9316787719726562 valid 0.8816146850585938\n",
      "EPOCH 2:\n",
      "LOSS train 1.5738961696624756 valid 0.972722053527832\n",
      "EPOCH 3:\n",
      "LOSS train 1.5487037897109985 valid 0.7620787620544434\n",
      "EPOCH 4:\n",
      "LOSS train 1.5083070993423462 valid 0.7861315011978149\n",
      "EPOCH 5:\n",
      "LOSS train 1.5102254152297974 valid 0.7813974618911743\n",
      "EPOCH 6:\n",
      "LOSS train 1.4687941074371338 valid 0.7645038962364197\n",
      "EPOCH 7:\n",
      "LOSS train 1.4683011770248413 valid 0.7768320441246033\n",
      "EPOCH 8:\n",
      "LOSS train 1.4309943914413452 valid 0.8291246294975281\n",
      "EPOCH 9:\n",
      "LOSS train 1.4389010667800903 valid 0.7303563356399536\n",
      "EPOCH 10:\n",
      "LOSS train 1.4529021978378296 valid 0.7007313966751099\n",
      "EPOCH 11:\n",
      "LOSS train 1.4765006303787231 valid 0.7550206184387207\n",
      "EPOCH 12:\n",
      "LOSS train 1.4414894580841064 valid 0.7208036780357361\n",
      "EPOCH 13:\n",
      "LOSS train 1.4368935823440552 valid 0.7191430926322937\n",
      "EPOCH 14:\n",
      "LOSS train 1.4087921380996704 valid 0.6958824396133423\n",
      "EPOCH 15:\n",
      "LOSS train 1.4345730543136597 valid 0.7516695857048035\n",
      "EPOCH 16:\n",
      "LOSS train 1.4219456911087036 valid 0.726151168346405\n",
      "EPOCH 17:\n",
      "LOSS train 1.4026405811309814 valid 0.6852986812591553\n",
      "EPOCH 18:\n",
      "LOSS train 1.4027049541473389 valid 0.6990102529525757\n",
      "EPOCH 19:\n",
      "LOSS train 1.4184306859970093 valid 0.7679719924926758\n",
      "EPOCH 20:\n",
      "LOSS train 1.4067071676254272 valid 0.6785609722137451\n",
      "EPOCH 21:\n",
      "LOSS train 1.4223182201385498 valid 0.6963298320770264\n",
      "EPOCH 22:\n",
      "LOSS train 1.405626654624939 valid 0.7626501321792603\n",
      "EPOCH 23:\n",
      "LOSS train 1.4176005125045776 valid 0.7802520990371704\n",
      "EPOCH 24:\n",
      "LOSS train 1.3999847173690796 valid 0.694509744644165\n",
      "EPOCH 25:\n",
      "LOSS train 1.4071671962738037 valid 0.7355639338493347\n",
      "EPOCH 26:\n",
      "LOSS train 1.412097454071045 valid 0.7636515498161316\n",
      "EPOCH 27:\n",
      "LOSS train 1.4255239963531494 valid 0.6945900917053223\n",
      "EPOCH 28:\n",
      "LOSS train 1.4353374242782593 valid 0.75592440366745\n",
      "EPOCH 29:\n",
      "LOSS train 1.4318699836730957 valid 0.6995347738265991\n",
      "EPOCH 30:\n",
      "LOSS train 1.4300637245178223 valid 0.7360109090805054\n",
      "EPOCH 31:\n",
      "LOSS train 1.4439071416854858 valid 0.6977258920669556\n",
      "EPOCH 32:\n",
      "LOSS train 1.4586368799209595 valid 0.7359480261802673\n",
      "EPOCH 33:\n",
      "LOSS train 1.4458461999893188 valid 0.7014528512954712\n",
      "EPOCH 34:\n",
      "LOSS train 1.4516507387161255 valid 0.78946852684021\n",
      "EPOCH 35:\n",
      "LOSS train 1.4637926816940308 valid 0.8384267687797546\n",
      "EPOCH 36:\n",
      "LOSS train 1.4239861965179443 valid 0.7332335710525513\n",
      "EPOCH 37:\n",
      "LOSS train 1.4211496114730835 valid 0.7542168498039246\n",
      "EPOCH 38:\n",
      "LOSS train 1.426427960395813 valid 0.7044706344604492\n",
      "EPOCH 39:\n",
      "LOSS train 1.4153931140899658 valid 0.7525181174278259\n",
      "EPOCH 40:\n",
      "LOSS train 1.4160759449005127 valid 0.6911076307296753\n",
      "EPOCH 41:\n",
      "LOSS train 1.4221900701522827 valid 0.7002972364425659\n",
      "EPOCH 42:\n",
      "LOSS train 1.4372318983078003 valid 0.7639386653900146\n",
      "EPOCH 43:\n",
      "LOSS train 1.4211289882659912 valid 0.767003059387207\n",
      "EPOCH 44:\n",
      "LOSS train 1.403170108795166 valid 0.7695584297180176\n",
      "EPOCH 45:\n",
      "LOSS train 1.4253374338150024 valid 0.6987892985343933\n",
      "EPOCH 46:\n",
      "LOSS train 1.4153468608856201 valid 0.7305468916893005\n",
      "EPOCH 47:\n",
      "LOSS train 1.420918345451355 valid 0.6834601163864136\n",
      "EPOCH 48:\n",
      "LOSS train 1.415361762046814 valid 0.7054672241210938\n",
      "EPOCH 49:\n",
      "LOSS train 1.4289335012435913 valid 0.7262374758720398\n",
      "EPOCH 50:\n",
      "LOSS train 1.4160805940628052 valid 0.7550258040428162\n",
      "FOLD:  4\n",
      "168\n",
      "48\n",
      "EPOCH 1:\n",
      "LOSS train 1.934463381767273 valid 1.4721111059188843\n",
      "EPOCH 2:\n",
      "LOSS train 1.552886962890625 valid 1.5302718877792358\n",
      "EPOCH 3:\n",
      "LOSS train 1.515816569328308 valid 1.3742661476135254\n",
      "EPOCH 4:\n",
      "LOSS train 1.487815499305725 valid 1.3591015338897705\n",
      "EPOCH 5:\n",
      "LOSS train 1.5221728086471558 valid 1.4251140356063843\n",
      "EPOCH 6:\n",
      "LOSS train 1.4981611967086792 valid 1.3961551189422607\n",
      "EPOCH 7:\n",
      "LOSS train 1.5032111406326294 valid 1.4373326301574707\n",
      "EPOCH 8:\n",
      "LOSS train 1.5046956539154053 valid 1.4778146743774414\n",
      "EPOCH 9:\n",
      "LOSS train 1.499587059020996 valid 1.3649804592132568\n",
      "EPOCH 10:\n",
      "LOSS train 1.4822287559509277 valid 1.460193157196045\n",
      "EPOCH 11:\n",
      "LOSS train 1.4771593809127808 valid 1.3779475688934326\n",
      "EPOCH 12:\n",
      "LOSS train 1.4843469858169556 valid 1.3631653785705566\n",
      "EPOCH 13:\n",
      "LOSS train 1.4682880640029907 valid 1.372503638267517\n",
      "EPOCH 14:\n",
      "LOSS train 1.4623184204101562 valid 1.4017202854156494\n",
      "EPOCH 15:\n",
      "LOSS train 1.451340675354004 valid 1.351520299911499\n",
      "EPOCH 16:\n",
      "LOSS train 1.4635242223739624 valid 1.3992915153503418\n",
      "EPOCH 17:\n",
      "LOSS train 1.4579336643218994 valid 1.343977928161621\n",
      "EPOCH 18:\n",
      "LOSS train 1.4545419216156006 valid 1.365710735321045\n",
      "EPOCH 19:\n",
      "LOSS train 1.4642523527145386 valid 1.3857167959213257\n",
      "EPOCH 20:\n",
      "LOSS train 1.4583953619003296 valid 1.432371973991394\n",
      "EPOCH 21:\n",
      "LOSS train 1.4784834384918213 valid 1.418219804763794\n",
      "EPOCH 22:\n",
      "LOSS train 1.4619684219360352 valid 1.4723987579345703\n",
      "EPOCH 23:\n",
      "LOSS train 1.4981330633163452 valid 1.4256062507629395\n",
      "EPOCH 24:\n",
      "LOSS train 1.4743952751159668 valid 1.3781003952026367\n",
      "EPOCH 25:\n",
      "LOSS train 1.4674421548843384 valid 1.4263265132904053\n",
      "EPOCH 26:\n",
      "LOSS train 1.4643105268478394 valid 1.394958257675171\n",
      "EPOCH 27:\n",
      "LOSS train 1.4474681615829468 valid 1.3715009689331055\n",
      "EPOCH 28:\n",
      "LOSS train 1.4452323913574219 valid 1.3571271896362305\n",
      "EPOCH 29:\n",
      "LOSS train 1.4380464553833008 valid 1.3546336889266968\n",
      "EPOCH 30:\n",
      "LOSS train 1.4439817667007446 valid 1.3974332809448242\n",
      "EPOCH 31:\n",
      "LOSS train 1.4250633716583252 valid 1.3239234685897827\n",
      "EPOCH 32:\n",
      "LOSS train 1.4444011449813843 valid 1.4106688499450684\n",
      "EPOCH 33:\n",
      "LOSS train 1.4459497928619385 valid 1.3956491947174072\n",
      "EPOCH 34:\n",
      "LOSS train 1.4631626605987549 valid 1.438376784324646\n",
      "EPOCH 35:\n",
      "LOSS train 1.4421730041503906 valid 1.3128952980041504\n",
      "EPOCH 36:\n",
      "LOSS train 1.44563627243042 valid 1.3762825727462769\n",
      "EPOCH 37:\n",
      "LOSS train 1.4409523010253906 valid 1.3294939994812012\n",
      "EPOCH 38:\n",
      "LOSS train 1.4624998569488525 valid 1.3893301486968994\n",
      "EPOCH 39:\n",
      "LOSS train 1.447803258895874 valid 1.3491086959838867\n",
      "EPOCH 40:\n",
      "LOSS train 1.4286117553710938 valid 1.3776987791061401\n",
      "EPOCH 41:\n",
      "LOSS train 1.4310916662216187 valid 1.3791253566741943\n",
      "EPOCH 42:\n",
      "LOSS train 1.4418063163757324 valid 1.3288843631744385\n",
      "EPOCH 43:\n",
      "LOSS train 1.412524700164795 valid 1.3487685918807983\n",
      "EPOCH 44:\n",
      "LOSS train 1.43384850025177 valid 1.4312251806259155\n",
      "EPOCH 45:\n",
      "LOSS train 1.4217712879180908 valid 1.3534071445465088\n",
      "EPOCH 46:\n",
      "LOSS train 1.4281736612319946 valid 1.3700485229492188\n",
      "EPOCH 47:\n",
      "LOSS train 1.4329981803894043 valid 1.386040210723877\n",
      "EPOCH 48:\n",
      "LOSS train 1.4424684047698975 valid 1.3839454650878906\n",
      "EPOCH 49:\n",
      "LOSS train 1.43678879737854 valid 1.332608699798584\n",
      "EPOCH 50:\n",
      "LOSS train 1.4478473663330078 valid 1.3954434394836426\n",
      "FOLD:  5\n",
      "172\n",
      "44\n",
      "EPOCH 1:\n",
      "LOSS train 1.9651122093200684 valid 0.26593947410583496\n",
      "EPOCH 2:\n",
      "LOSS train 1.56226646900177 valid 0.6971598863601685\n",
      "EPOCH 3:\n",
      "LOSS train 1.4947599172592163 valid 0.27724671363830566\n",
      "EPOCH 4:\n",
      "LOSS train 1.4317489862442017 valid 0.2833711802959442\n",
      "EPOCH 5:\n",
      "LOSS train 1.4261568784713745 valid 0.2572583258152008\n",
      "EPOCH 6:\n",
      "LOSS train 1.4435381889343262 valid 0.236768901348114\n",
      "EPOCH 7:\n",
      "LOSS train 1.4297009706497192 valid 0.28368833661079407\n",
      "EPOCH 8:\n",
      "LOSS train 1.4100492000579834 valid 0.20213401317596436\n",
      "EPOCH 9:\n",
      "LOSS train 1.4147026538848877 valid 0.2123621702194214\n",
      "EPOCH 10:\n",
      "LOSS train 1.3998064994812012 valid 0.22021552920341492\n",
      "EPOCH 11:\n",
      "LOSS train 1.4044221639633179 valid 0.2787514925003052\n",
      "EPOCH 12:\n",
      "LOSS train 1.4100936651229858 valid 0.22188979387283325\n",
      "EPOCH 13:\n",
      "LOSS train 1.3992286920547485 valid 0.24374699592590332\n",
      "EPOCH 14:\n",
      "LOSS train 1.3795628547668457 valid 0.194420725107193\n",
      "EPOCH 15:\n",
      "LOSS train 1.3793721199035645 valid 0.23219597339630127\n",
      "EPOCH 16:\n",
      "LOSS train 1.403083086013794 valid 0.2015552520751953\n",
      "EPOCH 17:\n",
      "LOSS train 1.3678148984909058 valid 0.1987241506576538\n",
      "EPOCH 18:\n",
      "LOSS train 1.3577477931976318 valid 0.2129189670085907\n",
      "EPOCH 19:\n",
      "LOSS train 1.3478429317474365 valid 0.23464414477348328\n",
      "EPOCH 20:\n",
      "LOSS train 1.3610330820083618 valid 0.24471235275268555\n",
      "EPOCH 21:\n",
      "LOSS train 1.3673090934753418 valid 0.22989299893379211\n",
      "EPOCH 22:\n",
      "LOSS train 1.3851170539855957 valid 0.23679104447364807\n",
      "EPOCH 23:\n",
      "LOSS train 1.3853504657745361 valid 0.2215510457754135\n",
      "EPOCH 24:\n",
      "LOSS train 1.424489140510559 valid 0.22707319259643555\n",
      "EPOCH 25:\n",
      "LOSS train 1.4536679983139038 valid 0.24294507503509521\n",
      "EPOCH 26:\n",
      "LOSS train 1.4238919019699097 valid 0.19812028110027313\n",
      "EPOCH 27:\n",
      "LOSS train 1.4067623615264893 valid 0.25299176573753357\n",
      "EPOCH 28:\n",
      "LOSS train 1.3773725032806396 valid 0.21606838703155518\n",
      "EPOCH 29:\n",
      "LOSS train 1.425909161567688 valid 0.24518410861492157\n",
      "EPOCH 30:\n",
      "LOSS train 1.427994966506958 valid 0.24799327552318573\n",
      "EPOCH 31:\n",
      "LOSS train 1.3949344158172607 valid 0.1993219256401062\n",
      "EPOCH 32:\n",
      "LOSS train 1.4150117635726929 valid 0.20972031354904175\n",
      "EPOCH 33:\n",
      "LOSS train 1.3800663948059082 valid 0.20286677777767181\n",
      "EPOCH 34:\n",
      "LOSS train 1.3728229999542236 valid 0.20425745844841003\n",
      "EPOCH 35:\n",
      "LOSS train 1.3730310201644897 valid 0.2370508313179016\n",
      "EPOCH 36:\n",
      "LOSS train 1.3986952304840088 valid 0.20839974284172058\n",
      "EPOCH 37:\n",
      "LOSS train 1.3803752660751343 valid 0.20369993150234222\n",
      "EPOCH 38:\n",
      "LOSS train 1.4030375480651855 valid 0.22322075068950653\n",
      "EPOCH 39:\n",
      "LOSS train 1.3819115161895752 valid 0.24217447638511658\n",
      "EPOCH 40:\n",
      "LOSS train 1.368067979812622 valid 0.23868921399116516\n",
      "EPOCH 41:\n",
      "LOSS train 1.3711860179901123 valid 0.22272197902202606\n",
      "EPOCH 42:\n",
      "LOSS train 1.3962547779083252 valid 0.21822145581245422\n",
      "EPOCH 43:\n",
      "LOSS train 1.3984304666519165 valid 0.24427402019500732\n",
      "EPOCH 44:\n",
      "LOSS train 1.4008584022521973 valid 0.23456788063049316\n",
      "EPOCH 45:\n",
      "LOSS train 1.3815604448318481 valid 0.21433976292610168\n",
      "EPOCH 46:\n",
      "LOSS train 1.3761385679244995 valid 0.2078709900379181\n",
      "EPOCH 47:\n",
      "LOSS train 1.3863005638122559 valid 0.21421661972999573\n",
      "EPOCH 48:\n",
      "LOSS train 1.3744890689849854 valid 0.23154959082603455\n",
      "EPOCH 49:\n",
      "LOSS train 1.399726390838623 valid 0.21999794244766235\n",
      "EPOCH 50:\n",
      "LOSS train 1.388446569442749 valid 0.2466493844985962\n"
     ]
    }
   ],
   "source": [
    "folds = cross_fold.create_folded_datasets(\"../data/label_info/labels.json\")\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "batch_size = 13\n",
    "\n",
    "root_dir = '../data/resized_224X224'\n",
    "\n",
    "json_file = '../data/label_info/refined_patient_wise_images.json'\n",
    "\n",
    "annotations_file = 'MIDRC mRALE Mastermind Training Annotations_2079_20230428.csv'\n",
    "\n",
    "experiment_name = 'ResNet-50 Contrastive Pretraining Per Fold'\n",
    "\n",
    "ex_directory = os.path.join('experiments',experiment_name)\n",
    "if not os.path.exists(ex_directory):\n",
    "        os.makedirs(ex_directory)\n",
    "\n",
    "models_directory = os.path.join(ex_directory,'saved_models')\n",
    "if not os.path.exists(models_directory):\n",
    "        os.makedirs(models_directory)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def make_contrastive_pretrained_model():\n",
    "        model = torchvision.models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
    "        model.conv1 = torch.nn.Conv2d(1,64, kernel_size=(7,7),stride=(2,2),padding=(3,3), bias=False)\n",
    "        model.fc = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(2048, 512, bias=True),\n",
    "                    torch.nn.Linear(512, 256, bias=True))\n",
    "        model.to(device)\n",
    "        return model\n",
    "\n",
    "criterion = SimCLR_Loss(batch_size, temperature=0.5)\n",
    "\n",
    "saved_metrics = []\n",
    "\n",
    "for f_i,fold in enumerate(folds):\n",
    "        print(\"FOLD: \",f_i+1)\n",
    "        train_list, val_list = fold\n",
    "\n",
    "        model = make_contrastive_pretrained_model()\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001)\n",
    "\n",
    "        train_dataset = midrc_SIMCLR_dataset(root_dir, annotations_file,json_file, transform = transform, fp_list = train_list)\n",
    "        val_dataset = midrc_SIMCLR_dataset(root_dir, annotations_file,json_file, transform=transform, fp_list = val_list)\n",
    "\n",
    "        print(len(train_dataset))\n",
    "        print(len(val_dataset))\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "        #Training per fold\n",
    "        metrics = train(epochs,model,device,train_loader,val_loader, criterion, optimizer,experiment_name, fold_number=f_i+1) # Folds will be started from 1 instead of 0\n",
    "        saved_metrics.append(metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrale",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
