{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from midrc_dataset import midrc_SIMCLR_dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from get_model import create_model\n",
    "import torchvision\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cross_fold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLR_Loss(nn.Module):\n",
    "    def __init__(self, batch_size, temperature):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "\n",
    "        #self.mask = self.mask_correlated_samples(batch_size)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        self.similarity_f = nn.CosineSimilarity(dim=2)\n",
    "\n",
    "    def mask_correlated_samples(self, batch_size):\n",
    "        N = 2 * batch_size\n",
    "        mask = torch.ones((N, N), dtype=bool)\n",
    "        mask = mask.fill_diagonal_(0)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            mask[i, batch_size + i] = 0\n",
    "            mask[batch_size + i, i] = 0\n",
    "        return mask\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "\n",
    "        mask = self.mask_correlated_samples(z_i.shape[0])\n",
    "\n",
    "        N = 2 * z_i.shape[0]\n",
    "\n",
    "        z = torch.cat((z_i, z_j), dim=0)\n",
    "\n",
    "        sim = self.similarity_f(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n",
    "\n",
    "        sim_i_j = torch.diag(sim, z_i.shape[0])\n",
    "        sim_j_i = torch.diag(sim, -z_i.shape[0])\n",
    "        \n",
    "        # We have 2N samples, but with Distributed training every GPU gets N examples too, resulting in: 2xNxN\n",
    "        #print(sim_i_j.shape, sim_j_i.shape)\n",
    "        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n",
    "        #print(\"Positive Done\")\n",
    "        negative_samples = sim[mask].reshape(N, -1)\n",
    "        #print(\"Negative Done\")\n",
    "        \n",
    "        #SIMCLR\n",
    "        labels = torch.from_numpy(np.array([0]*N)).reshape(-1).to(positive_samples.device).long() #.float()\n",
    "        \n",
    "        logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss /= N\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs,model,device, train_loader, val_loader, criterion, optimizer, experiment_name, fold_number):\n",
    "    metrics={}\n",
    "    best_vloss = 1_000_000.\n",
    "    for e in range(epochs):\n",
    "        print('EPOCH {}:'.format(e + 1))\n",
    "        running_loss = 0.\n",
    "        last_loss = 0.\n",
    "        running_train_kappa=0.0\n",
    "        avg_train_kappa=0.0\n",
    "        model.train(True)\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            # Every data instance is an input + label pair\n",
    "            img1, img2, score1, score2 = data\n",
    "\n",
    "            #print(\"img1 and img2 shape\", img1.shape, img2.shape)\n",
    "\n",
    "            # Zero your gradients for every batch!\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Make predictions for this batch\n",
    "            output1 = model(img1.to(device))\n",
    "            output2 = model(img2.to(device))\n",
    "\n",
    "            # Compute the loss and its gradients\n",
    "            loss = criterion(output1, output2) \n",
    "            loss.backward()\n",
    "\n",
    "            # Adjust learning weights\n",
    "            optimizer.step()\n",
    "        \n",
    "            # Gather data\n",
    "            running_loss += loss\n",
    "        \n",
    "        avg_loss = running_loss/(i+1)\n",
    "        \n",
    "        running_vloss = 0.0\n",
    "        running_kappa = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(val_loader):\n",
    "                img1, img2, score1, score2 = vdata\n",
    "                voutput1 = model(img1.to(device))\n",
    "                voutput2 = model(img2.to(device))\n",
    "                vloss = criterion(output1,output2) # TODO: Create cosine sim loss\n",
    "                running_vloss += vloss\n",
    "\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        \n",
    "        print('LOSS train {} valid {}'.format(avg_loss,avg_vloss))\n",
    "        metrics[e] = {      \n",
    "            f'Fold {fold_number} avg_train_loss': avg_loss.item(),\n",
    "            f'Fold {fold_number} avg_val_loss': avg_vloss.item(),\n",
    "            f'Fold {fold_number} epoch': e\n",
    "        }\n",
    "        #if config['wandb']==True:\n",
    "        #    wandb.log(metrics[e])\n",
    "\n",
    "        # Track best performance, and save the model's state\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = os.path.join('experiments',experiment_name,'saved_models', 'modelsave_fold_{}.ckpt'.format(fold_number))\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1  Train Indices:  2078  Val indices:  520\n",
      "Fold:  2  Train Indices:  2096  Val indices:  502\n",
      "Fold:  3  Train Indices:  2050  Val indices:  548\n",
      "Fold:  4  Train Indices:  2072  Val indices:  526\n",
      "Fold:  5  Train Indices:  2096  Val indices:  502\n",
      "FOLD:  1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'make_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFOLD: \u001b[39m\u001b[39m\"\u001b[39m,f_i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     43\u001b[0m train_list, val_list \u001b[39m=\u001b[39m fold\n\u001b[1;32m---> 45\u001b[0m model \u001b[39m=\u001b[39m make_model()\n\u001b[0;32m     47\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdamW(model\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m)\n\u001b[0;32m     49\u001b[0m train_dataset \u001b[39m=\u001b[39m midrc_SIMCLR_dataset(root_dir, annotations_file,json_file, transform \u001b[39m=\u001b[39m transform, fp_list \u001b[39m=\u001b[39m train_list)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'make_model' is not defined"
     ]
    }
   ],
   "source": [
    "folds = cross_fold.create_folded_datasets(\"../data/label_info/labels.json\")\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "batch_size = 13\n",
    "\n",
    "root_dir = '../data/resized_224X224'\n",
    "\n",
    "json_file = '../data/label_info/refined_patient_wise_images.json'\n",
    "\n",
    "annotations_file = 'MIDRC mRALE Mastermind Training Annotations_2079_20230428.csv'\n",
    "\n",
    "experiment_name = 'MIDRC Contrastive Pretraining Per Fold'\n",
    "\n",
    "ex_directory = os.path.join('experiments',experiment_name)\n",
    "if not os.path.exists(ex_directory):\n",
    "        os.makedirs(ex_directory)\n",
    "\n",
    "models_directory = os.path.join(ex_directory,'saved_models')\n",
    "if not os.path.exists(models_directory):\n",
    "        os.makedirs(models_directory)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def make_contrastive_pretrained_model():\n",
    "        model = torchvision.models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
    "        model.conv1 = torch.nn.Conv2d(1,64, kernel_size=(7,7),stride=(2,2),padding=(3,3), bias=False)\n",
    "        model.fc = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(2048, 512, bias=True),\n",
    "                    torch.nn.Linear(512, 256, bias=True))\n",
    "        model.to(device)\n",
    "        return model\n",
    "\n",
    "criterion = SimCLR_Loss(batch_size, temperature=0.5)\n",
    "\n",
    "saved_metrics = []\n",
    "\n",
    "for f_i,fold in enumerate(folds):\n",
    "        print(\"FOLD: \",f_i+1)\n",
    "        train_list, val_list = fold\n",
    "\n",
    "        model = make_model()\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001)\n",
    "\n",
    "        train_dataset = midrc_SIMCLR_dataset(root_dir, annotations_file,json_file, transform = transform, fp_list = train_list)\n",
    "        val_dataset = midrc_SIMCLR_dataset(root_dir, annotations_file,json_file, transform=transform, fp_list = val_list)\n",
    "\n",
    "        print(len(train_dataset))\n",
    "        print(len(val_dataset))\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "        #Training per fold\n",
    "        metrics = train(epochs,model,device,train_loader,val_loader, criterion, optimizer,experiment_name, fold_number=f_i+1) # Folds will be started from 1 instead of 0\n",
    "        saved_metrics.append(metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrale",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
