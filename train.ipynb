{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import cross_fold\n",
    "from midrc_dataset import midrc_challenge_dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "import os\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from get_model import create_model\n",
    "from config import config\n",
    "import numpy as np\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Train on 1 fold (1 dataset) with the input settings\n",
    "\n",
    "    Returns: dict of training metrics\n",
    "             best validation model is saved to \\models\n",
    "'''\n",
    "def train(epochs,model,device, train_loader, val_loader, criterion, optimizer, fold_number):\n",
    "    metrics={}\n",
    "    best_vloss = 1_000_000.\n",
    "    for e in range(epochs):\n",
    "        print('EPOCH {}:'.format(e + 1))\n",
    "        running_loss = 0.\n",
    "        last_loss = 0.\n",
    "        running_train_kappa=0.0\n",
    "        avg_train_kappa=0.0\n",
    "        model.train(True)\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            # Every data instance is an input + label pair\n",
    "            inputs, labels = data\n",
    "\n",
    "            # Zero your gradients for every batch!\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Make predictions for this batch\n",
    "            outputs = model(inputs.to(device))\n",
    "            labels=labels/24.0\n",
    "\n",
    "            # Compute the loss and its gradients\n",
    "            loss = criterion(outputs, labels.float().to(device).unsqueeze(1))\n",
    "            loss.backward()\n",
    "\n",
    "            # Adjust learning weights\n",
    "            optimizer.step()\n",
    "            outputs=outputs*24.0\n",
    "            labels=labels*24.0\n",
    "            \n",
    "            outputs=torch.round(outputs)\n",
    "            outputs=outputs.data.cpu().numpy()\n",
    "            \n",
    "            labels=labels.data.cpu().numpy()\n",
    "            outputs=outputs.flatten()\n",
    "            labels=labels.flatten()\n",
    "            kappa_score_train=cohen_kappa_score(labels,outputs,weights='quadratic')\n",
    "            running_train_kappa+=kappa_score_train\n",
    "            # Gather data\n",
    "            running_loss += loss\n",
    "\n",
    "\n",
    "        \n",
    "        avg_loss = running_loss/(i+1)\n",
    "        avg_train_kappa=running_train_kappa/(i+1)\n",
    "\n",
    "        \n",
    "        running_vloss = 0.0\n",
    "        running_kappa = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(val_loader):\n",
    "                vinputs, vlabels = vdata\n",
    "                voutputs = model(vinputs.to(device))\n",
    "                vlabels=vlabels/24.0\n",
    "                vloss = criterion(voutputs, vlabels.float().to(device).unsqueeze(1))\n",
    "                voutputs=voutputs*24.0\n",
    "                vlabels=vlabels*24.0\n",
    "                voutputs=torch.round(voutputs)\n",
    "                voutputs=voutputs.data.cpu().numpy()\n",
    "                vlabels=vlabels.data.cpu().numpy()\n",
    "                voutputs=voutputs.flatten()\n",
    "                vlabels=vlabels.flatten()\n",
    "                kappa_score=cohen_kappa_score(vlabels,voutputs,weights='quadratic')\n",
    "                running_kappa+=kappa_score\n",
    "                running_vloss += vloss\n",
    "                \n",
    "\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        avg_val_kappa= running_kappa / (i + 1)\n",
    "\n",
    "        #Print and Log Metrics\n",
    "        print('LOSS train {} valid {}, Kappa train {} valid {}'.format(avg_loss,avg_vloss,avg_train_kappa,avg_val_kappa))\n",
    "        metrics[e] = {      \n",
    "            'avg_loss': avg_loss.item(),\n",
    "            'avg_vloss': avg_vloss.item(),\n",
    "            'avg_train_kappa': avg_train_kappa,\n",
    "            'avg_val_kappa': avg_val_kappa\n",
    "        }   \n",
    "\n",
    "        # Track best performance, and save the model's state\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = os.path.join('models','modelsave_fold_{}'.format(fold_number))\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,device,dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        squared_error = 0\n",
    "        absolute_error = 0\n",
    "        s_percent_error = 0\n",
    "        kappa = 0\n",
    "        kappa_list_y1 = []\n",
    "        kappa_list_y2 = []\n",
    "\n",
    "        for data in dataloader:\n",
    "            inputs, labels = data\n",
    "\n",
    "            outputs = model(inputs.to(device))\n",
    "            labels=labels/24.0\n",
    "\n",
    "            # For error metrics we normalize labels and outputs to 0-1\n",
    "            squared_error += (outputs - labels.to(device))**2\n",
    "            absolute_error += torch.abs(outputs-labels.to(device))\n",
    "            s_percent_error += torch.abs((outputs-labels.to(device))/((labels.to(device) + outputs)/2))\n",
    "\n",
    "\n",
    "            outputs=outputs*24.0\n",
    "            labels=labels*24.0\n",
    "            outputs=torch.round(outputs)\n",
    "            outputs=outputs.data.cpu().numpy()\n",
    "            labels=labels.data.cpu().numpy()\n",
    "            outputs=outputs.flatten()\n",
    "            labels=labels.flatten()\n",
    "\n",
    "            kappa_list_y1.extend(labels)\n",
    "            kappa_list_y2.extend(outputs)\n",
    "\n",
    "\n",
    "        kappa = cohen_kappa_score(kappa_list_y1,kappa_list_y2,weights='quadratic')\n",
    "            \n",
    "        #returns mae and rmse\n",
    "        return absolute_error/len(dataloader), torch.sqrt(squared_error/len(dataloader)), s_percent_error/len(dataloader), kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1  Train Indices:  2078  Val indices:  520\n",
      "Fold:  2  Train Indices:  2096  Val indices:  502\n",
      "Fold:  3  Train Indices:  2050  Val indices:  548\n",
      "Fold:  4  Train Indices:  2072  Val indices:  526\n",
      "Fold:  5  Train Indices:  2096  Val indices:  502\n",
      "FOLD:  1\n",
      "EPOCH 1:\n",
      "LOSS train 0.09837006032466888 valid 0.08274707198143005, Kappa train 0.3605961501120536 valid 0.6380862346837433\n",
      "EPOCH 2:\n",
      "LOSS train 0.08133675903081894 valid 0.14117181301116943, Kappa train 0.5028131605988656 valid 0.08388937029432218\n",
      "EPOCH 3:\n",
      "LOSS train 0.06746343523263931 valid 0.1113230437040329, Kappa train 0.594586264962493 valid 0.522293456525292\n",
      "EPOCH 4:\n",
      "LOSS train 0.05264813452959061 valid 0.06474318355321884, Kappa train 0.7003659616476401 valid 0.6458960883875163\n",
      "EPOCH 5:\n",
      "LOSS train 0.0454980805516243 valid 0.05385035276412964, Kappa train 0.7448901710420097 valid 0.7062819804570243\n",
      "EPOCH 6:\n",
      "LOSS train 0.04292737692594528 valid 0.09129422903060913, Kappa train 0.7496059309637554 valid 0.5019915269100044\n",
      "EPOCH 7:\n",
      "LOSS train 0.03799006715416908 valid 0.07728708535432816, Kappa train 0.7834336147564046 valid 0.6670640410574522\n",
      "EPOCH 8:\n",
      "LOSS train 0.03208259120583534 valid 0.05877886712551117, Kappa train 0.8123353588588628 valid 0.6852652239765631\n",
      "EPOCH 9:\n",
      "LOSS train 0.026095690205693245 valid 0.05735892429947853, Kappa train 0.8387546705703133 valid 0.7382574654965219\n",
      "EPOCH 10:\n",
      "LOSS train 0.021612107753753662 valid 0.0636848658323288, Kappa train 0.8699812380631803 valid 0.7184863103470299\n",
      "EPOCH 11:\n",
      "LOSS train 0.017451956868171692 valid 0.05031853914260864, Kappa train 0.8824640194940297 valid 0.750713428764424\n",
      "EPOCH 12:\n",
      "LOSS train 0.016619080677628517 valid 0.053969185799360275, Kappa train 0.8879832837451446 valid 0.7461023544919936\n",
      "EPOCH 13:\n",
      "LOSS train 0.01444157212972641 valid 0.04988899081945419, Kappa train 0.8949223722914915 valid 0.7637614586758961\n",
      "EPOCH 14:\n",
      "LOSS train 0.012271135114133358 valid 0.044456228613853455, Kappa train 0.9123082741138536 valid 0.797994077189343\n",
      "EPOCH 15:\n",
      "LOSS train 0.010795065201818943 valid 0.04002390429377556, Kappa train 0.9208899457655705 valid 0.8094836877766122\n",
      "EPOCH 16:\n",
      "LOSS train 0.009929503314197063 valid 0.05066772550344467, Kappa train 0.9261799635112861 valid 0.7337150419935038\n",
      "EPOCH 17:\n",
      "LOSS train 0.00810788944363594 valid 0.04358861967921257, Kappa train 0.9357499783721605 valid 0.7904698189584632\n",
      "EPOCH 18:\n",
      "LOSS train 0.008840211667120457 valid 0.05833200737833977, Kappa train 0.9273197494478993 valid 0.7194650377417768\n",
      "EPOCH 19:\n",
      "LOSS train 0.008351013995707035 valid 0.04888831451535225, Kappa train 0.9333930176476748 valid 0.7703219939512936\n",
      "EPOCH 20:\n",
      "LOSS train 0.008921938017010689 valid 0.05077581852674484, Kappa train 0.9250845344176628 valid 0.7328769691242435\n",
      "EPOCH 21:\n",
      "LOSS train 0.007803839165717363 valid 0.04282074421644211, Kappa train 0.940156869096048 valid 0.7963102988902141\n",
      "EPOCH 22:\n",
      "LOSS train 0.006602196022868156 valid 0.04508320242166519, Kappa train 0.9468116495792077 valid 0.7729281637791925\n",
      "EPOCH 23:\n",
      "LOSS train 0.005683163180947304 valid 0.03998918831348419, Kappa train 0.9508293281958814 valid 0.8005240398582734\n",
      "EPOCH 24:\n",
      "LOSS train 0.005957686807960272 valid 0.04341461881995201, Kappa train 0.9473012384529867 valid 0.7769670238552983\n",
      "EPOCH 25:\n",
      "LOSS train 0.005491163115948439 valid 0.040160033851861954, Kappa train 0.9499842872689909 valid 0.7903263695495006\n",
      "EPOCH 26:\n",
      "LOSS train 0.004882126580923796 valid 0.04697202146053314, Kappa train 0.9559563592591666 valid 0.7757626247459791\n",
      "EPOCH 27:\n",
      "LOSS train 0.004705476574599743 valid 0.04343126341700554, Kappa train 0.9582110675733334 valid 0.7705623488855364\n",
      "EPOCH 28:\n",
      "LOSS train 0.0039778826758265495 valid 0.04312505945563316, Kappa train 0.9629380676472201 valid 0.7810091688707\n",
      "EPOCH 29:\n",
      "LOSS train 0.003999799489974976 valid 0.0426233671605587, Kappa train 0.9633897040813763 valid 0.7769911242104987\n",
      "EPOCH 30:\n",
      "LOSS train 0.004433269612491131 valid 0.04278777539730072, Kappa train 0.9581422552044838 valid 0.7920795845477612\n",
      "EPOCH 31:\n",
      "LOSS train 0.004325727466493845 valid 0.04213017225265503, Kappa train 0.959243904366471 valid 0.7827513121744418\n",
      "EPOCH 32:\n",
      "LOSS train 0.004318563267588615 valid 0.048872146755456924, Kappa train 0.9576009920444185 valid 0.7479737613224593\n",
      "EPOCH 33:\n",
      "LOSS train 0.0038880810607224703 valid 0.0463855043053627, Kappa train 0.9646971951620721 valid 0.7893882590727582\n",
      "EPOCH 34:\n",
      "LOSS train 0.0037972924765199423 valid 0.03988469019532204, Kappa train 0.9651457942752134 valid 0.7970346376034045\n",
      "EPOCH 35:\n",
      "LOSS train 0.003808896290138364 valid 0.04534986987709999, Kappa train 0.9650321212662198 valid 0.7654903826544514\n",
      "EPOCH 36:\n",
      "LOSS train 0.003510389244183898 valid 0.04184484854340553, Kappa train 0.967448457561197 valid 0.7957734452207871\n",
      "EPOCH 37:\n",
      "LOSS train 0.0034232926554977894 valid 0.04616159945726395, Kappa train 0.9670296247237998 valid 0.7644085229475132\n",
      "EPOCH 38:\n",
      "LOSS train 0.003593559144064784 valid 0.04060465097427368, Kappa train 0.9684126275286536 valid 0.7781515304148827\n",
      "EPOCH 39:\n",
      "LOSS train 0.003880507778376341 valid 0.05084219574928284, Kappa train 0.9638183367034099 valid 0.7176016408778066\n",
      "EPOCH 40:\n",
      "LOSS train 0.0037791822105646133 valid 0.043736930936574936, Kappa train 0.9659811420465525 valid 0.7613235224217982\n",
      "EPOCH 41:\n",
      "LOSS train 0.005053107626736164 valid 0.056029994040727615, Kappa train 0.9558995177796201 valid 0.7270979861845006\n",
      "EPOCH 42:\n",
      "LOSS train 0.007464494556188583 valid 0.06762558966875076, Kappa train 0.9393185851368088 valid 0.7088583416800665\n",
      "EPOCH 43:\n",
      "LOSS train 0.010837375186383724 valid 0.07043769210577011, Kappa train 0.9238603919862641 valid 0.7138660581467026\n",
      "EPOCH 44:\n",
      "LOSS train 0.00919719971716404 valid 0.05924604460597038, Kappa train 0.932246075445212 valid 0.6863086090288774\n",
      "EPOCH 45:\n",
      "LOSS train 0.006070059724152088 valid 0.0554099939763546, Kappa train 0.9486573052242532 valid 0.7200236861424251\n",
      "EPOCH 46:\n",
      "LOSS train 0.004720779601484537 valid 0.044023752212524414, Kappa train 0.9626236412096356 valid 0.7592202495612917\n",
      "EPOCH 47:\n",
      "LOSS train 0.0034420921001583338 valid 0.042458344250917435, Kappa train 0.9666286393205152 valid 0.783993632472108\n",
      "EPOCH 48:\n",
      "LOSS train 0.0025286453310400248 valid 0.04242859035730362, Kappa train 0.9734551323453007 valid 0.786619037490414\n",
      "EPOCH 49:\n",
      "LOSS train 0.0019732904620468616 valid 0.04268072545528412, Kappa train 0.9761733167211706 valid 0.7905429398372763\n",
      "EPOCH 50:\n",
      "LOSS train 0.001635245163924992 valid 0.03872394189238548, Kappa train 0.9812153007815928 valid 0.7939751054738089\n",
      "FOLD:  2\n",
      "EPOCH 1:\n",
      "LOSS train 0.09598983824253082 valid 0.08914883434772491, Kappa train 0.4647784859539221 valid 0.3972772159173286\n",
      "EPOCH 2:\n",
      "LOSS train 0.07722032815217972 valid 0.10496485233306885, Kappa train 0.5771660552583434 valid 0.17770773626026942\n",
      "EPOCH 3:\n",
      "LOSS train 0.06321505457162857 valid 0.09551772475242615, Kappa train 0.658963082230749 valid 0.4615051882150508\n",
      "EPOCH 4:\n",
      "LOSS train 0.05458325147628784 valid 0.07158401608467102, Kappa train 0.7112422692970444 valid 0.46872601688560495\n",
      "EPOCH 5:\n",
      "LOSS train 0.05070822313427925 valid 0.05627966672182083, Kappa train 0.7369947311409359 valid 0.6002473492278075\n",
      "EPOCH 6:\n",
      "LOSS train 0.040749143809080124 valid 0.05320582166314125, Kappa train 0.7793872089945625 valid 0.6045424835720438\n",
      "EPOCH 7:\n",
      "LOSS train 0.034619349986314774 valid 0.08205538988113403, Kappa train 0.8171151180696372 valid 0.5358726438839012\n",
      "EPOCH 8:\n",
      "LOSS train 0.029064903035759926 valid 0.06334973871707916, Kappa train 0.8320312073208604 valid 0.6197671171512265\n",
      "EPOCH 9:\n",
      "LOSS train 0.024735817685723305 valid 0.06289136409759521, Kappa train 0.8486190512945365 valid 0.6491977412630042\n",
      "EPOCH 10:\n",
      "LOSS train 0.025280682370066643 valid 0.09358418732881546, Kappa train 0.8520295375240976 valid 0.44539893610750786\n",
      "EPOCH 11:\n",
      "LOSS train 0.02221503295004368 valid 0.05293433368206024, Kappa train 0.861768611033959 valid 0.6388165005421943\n",
      "EPOCH 12:\n",
      "LOSS train 0.01639029011130333 valid 0.053553175181150436, Kappa train 0.8885902857054033 valid 0.668531879051274\n",
      "EPOCH 13:\n",
      "LOSS train 0.01237384881824255 valid 0.04742830619215965, Kappa train 0.911207670004806 valid 0.6928681890912819\n",
      "EPOCH 14:\n",
      "LOSS train 0.009851500391960144 valid 0.04925469681620598, Kappa train 0.9242816026040055 valid 0.6722888459715923\n",
      "EPOCH 15:\n",
      "LOSS train 0.00885184295475483 valid 0.04762721434235573, Kappa train 0.9289354665603026 valid 0.6734429430025282\n",
      "EPOCH 16:\n",
      "LOSS train 0.008304223418235779 valid 0.05203240364789963, Kappa train 0.9366626756502147 valid 0.6549759508133461\n",
      "EPOCH 17:\n",
      "LOSS train 0.007500093895941973 valid 0.04625987634062767, Kappa train 0.9384422503243627 valid 0.6743375042903752\n",
      "EPOCH 18:\n",
      "LOSS train 0.006612754426896572 valid 0.050413940101861954, Kappa train 0.9431447932829341 valid 0.6739003786360884\n",
      "EPOCH 19:\n",
      "LOSS train 0.0066201030276715755 valid 0.044467486441135406, Kappa train 0.9435101041829063 valid 0.6966822381357093\n",
      "EPOCH 20:\n",
      "LOSS train 0.005642014555633068 valid 0.044396717101335526, Kappa train 0.9524119951573434 valid 0.6871874068040965\n",
      "EPOCH 21:\n",
      "LOSS train 0.005856384988874197 valid 0.04717789962887764, Kappa train 0.9475426532689094 valid 0.6921824971609167\n",
      "EPOCH 22:\n",
      "LOSS train 0.005721421912312508 valid 0.04622400552034378, Kappa train 0.948261114036127 valid 0.6865960104718988\n",
      "EPOCH 23:\n",
      "LOSS train 0.004575611557811499 valid 0.04545341059565544, Kappa train 0.9585628086940807 valid 0.6990173935256593\n",
      "EPOCH 24:\n",
      "LOSS train 0.004935762844979763 valid 0.046507302671670914, Kappa train 0.9574297124665854 valid 0.6731353835929592\n",
      "EPOCH 25:\n",
      "LOSS train 0.005647340789437294 valid 0.044917263090610504, Kappa train 0.9544527538584279 valid 0.6833636921588863\n",
      "EPOCH 26:\n",
      "LOSS train 0.006478859111666679 valid 0.06359315663576126, Kappa train 0.9450556097298239 valid 0.6021703782788245\n",
      "EPOCH 27:\n",
      "LOSS train 0.006010459270328283 valid 0.04461958631873131, Kappa train 0.9459629643160103 valid 0.67861881258737\n",
      "EPOCH 28:\n",
      "LOSS train 0.004977724514901638 valid 0.0469435378909111, Kappa train 0.9578325964161972 valid 0.6878605575206538\n",
      "EPOCH 29:\n",
      "LOSS train 0.004983673337846994 valid 0.04693317040801048, Kappa train 0.9559475249633713 valid 0.6707731068823819\n",
      "EPOCH 30:\n",
      "LOSS train 0.00568270543590188 valid 0.04528002813458443, Kappa train 0.9518489287771899 valid 0.6830858161310771\n",
      "EPOCH 31:\n",
      "LOSS train 0.00569113390520215 valid 0.04894430562853813, Kappa train 0.9542469626423415 valid 0.7103518869592157\n",
      "EPOCH 32:\n",
      "LOSS train 0.005822100210934877 valid 0.04242803156375885, Kappa train 0.9533077036351303 valid 0.6893873812815259\n",
      "EPOCH 33:\n",
      "LOSS train 0.007662556599825621 valid 0.05157426372170448, Kappa train 0.9434071511660367 valid 0.6131227182808002\n",
      "EPOCH 34:\n",
      "LOSS train 0.005791465286165476 valid 0.04439874738454819, Kappa train 0.9524951136150939 valid 0.6856512613014654\n",
      "EPOCH 35:\n",
      "LOSS train 0.004897122271358967 valid 0.043851085007190704, Kappa train 0.9598174625054833 valid 0.6967936271925135\n",
      "EPOCH 36:\n",
      "LOSS train 0.0044763959012925625 valid 0.043808598071336746, Kappa train 0.9608491458414209 valid 0.7103107201578986\n",
      "EPOCH 37:\n",
      "LOSS train 0.0032467953860759735 valid 0.0421719066798687, Kappa train 0.9701954618584122 valid 0.7057204788443625\n",
      "EPOCH 38:\n",
      "LOSS train 0.002895005512982607 valid 0.04388159140944481, Kappa train 0.970970960163314 valid 0.6780753916498297\n",
      "EPOCH 39:\n",
      "LOSS train 0.0025889738462865353 valid 0.04057304933667183, Kappa train 0.973803524100311 valid 0.7031907461508103\n",
      "EPOCH 40:\n",
      "LOSS train 0.0026288034860044718 valid 0.045985035598278046, Kappa train 0.9725365216894407 valid 0.7056503000764269\n",
      "EPOCH 41:\n",
      "LOSS train 0.0024741743691265583 valid 0.04496106132864952, Kappa train 0.9715940258255095 valid 0.6874436068571842\n",
      "EPOCH 42:\n",
      "LOSS train 0.0022504297085106373 valid 0.0434332974255085, Kappa train 0.9756355735333867 valid 0.6888711905241613\n",
      "EPOCH 43:\n",
      "LOSS train 0.002296004444360733 valid 0.042242176830768585, Kappa train 0.9754759542307223 valid 0.7103327295826047\n",
      "EPOCH 44:\n",
      "LOSS train 0.002284072572365403 valid 0.04211278632283211, Kappa train 0.9737745546282665 valid 0.694437680357751\n",
      "EPOCH 45:\n",
      "LOSS train 0.0019487449899315834 valid 0.04130854457616806, Kappa train 0.9790564218587174 valid 0.7131248744601902\n",
      "EPOCH 46:\n",
      "LOSS train 0.0020438446663320065 valid 0.04257570579648018, Kappa train 0.9788468157553991 valid 0.6700633980704906\n",
      "EPOCH 47:\n",
      "LOSS train 0.002297670114785433 valid 0.04350203275680542, Kappa train 0.973668464598999 valid 0.6829538194332743\n",
      "EPOCH 48:\n",
      "LOSS train 0.002292263088747859 valid 0.044910166412591934, Kappa train 0.9752086165916296 valid 0.688043375373193\n",
      "EPOCH 49:\n",
      "LOSS train 0.0026683544274419546 valid 0.04725613445043564, Kappa train 0.9733108117580098 valid 0.6652639771638577\n",
      "EPOCH 50:\n",
      "LOSS train 0.01891213096678257 valid 0.447986364364624, Kappa train 0.8909000362528006 valid -0.15608913562650728\n",
      "FOLD:  3\n",
      "EPOCH 1:\n",
      "LOSS train 0.07886107265949249 valid 0.0733514130115509, Kappa train 0.5368738120319181 valid 0.6124970390860311\n",
      "EPOCH 2:\n",
      "LOSS train 0.05242147296667099 valid 0.05149054527282715, Kappa train 0.7028888579062182 valid 0.7598384103284754\n",
      "EPOCH 3:\n",
      "LOSS train 0.04908229410648346 valid 0.07206317782402039, Kappa train 0.7326861568402532 valid 0.6496062603880056\n",
      "EPOCH 4:\n",
      "LOSS train 0.03891966864466667 valid 0.13601742684841156, Kappa train 0.7756398661270861 valid 0.5235968067594521\n",
      "EPOCH 5:\n",
      "LOSS train 0.038352709263563156 valid 0.08552976697683334, Kappa train 0.7865391824555009 valid 0.6127808933404186\n",
      "EPOCH 6:\n",
      "LOSS train 0.03127341717481613 valid 0.06096883490681648, Kappa train 0.8151581513279883 valid 0.6726353680627165\n",
      "EPOCH 7:\n",
      "LOSS train 0.024835918098688126 valid 0.04241238534450531, Kappa train 0.8436496542733068 valid 0.7763017973719136\n",
      "EPOCH 8:\n",
      "LOSS train 0.023447515442967415 valid 0.057074323296546936, Kappa train 0.8439657874018439 valid 0.713738088124987\n",
      "EPOCH 9:\n",
      "LOSS train 0.01687033101916313 valid 0.08365997672080994, Kappa train 0.883032948829512 valid 0.6222657949706942\n",
      "EPOCH 10:\n",
      "LOSS train 0.01464369148015976 valid 0.07826477289199829, Kappa train 0.8998249902699231 valid 0.6113153124015281\n",
      "EPOCH 11:\n",
      "LOSS train 0.010604454204440117 valid 0.047533661127090454, Kappa train 0.9143889598026533 valid 0.7722530197205699\n",
      "EPOCH 12:\n",
      "LOSS train 0.010631825774908066 valid 0.04572891443967819, Kappa train 0.9197956024327967 valid 0.7556650312713596\n",
      "EPOCH 13:\n",
      "LOSS train 0.009842123836278915 valid 0.05381249636411667, Kappa train 0.9188671788623461 valid 0.7361458306256812\n",
      "EPOCH 14:\n",
      "LOSS train 0.008028264157474041 valid 0.04274391382932663, Kappa train 0.9239017018134511 valid 0.7805095508034947\n",
      "EPOCH 15:\n",
      "LOSS train 0.006677436176687479 valid 0.057957954704761505, Kappa train 0.9422892723781596 valid 0.7160389695859245\n",
      "EPOCH 16:\n",
      "LOSS train 0.017983511090278625 valid 0.053952910006046295, Kappa train 0.8802816348945254 valid 0.7099862362740689\n",
      "EPOCH 17:\n",
      "LOSS train 0.011126124300062656 valid 0.06556710600852966, Kappa train 0.9156929973171518 valid 0.6959053872720737\n",
      "EPOCH 18:\n",
      "LOSS train 0.010020410642027855 valid 0.04944686219096184, Kappa train 0.9194322649925656 valid 0.7260777702591261\n",
      "EPOCH 19:\n",
      "LOSS train 0.007550104986876249 valid 0.04952315241098404, Kappa train 0.9319686440934649 valid 0.770825660618868\n",
      "EPOCH 20:\n",
      "LOSS train 0.005856616422533989 valid 0.04358909651637077, Kappa train 0.9472878318309185 valid 0.7676687810614404\n",
      "EPOCH 21:\n",
      "LOSS train 0.0065653664059937 valid 0.0494358092546463, Kappa train 0.9415901217201273 valid 0.7957168500418692\n",
      "EPOCH 22:\n",
      "LOSS train 0.004463832359761 valid 0.04720548167824745, Kappa train 0.957296544670093 valid 0.7456464680886875\n",
      "EPOCH 23:\n",
      "LOSS train 0.003969141747802496 valid 0.04918637126684189, Kappa train 0.9583075752202798 valid 0.762976042790021\n",
      "EPOCH 24:\n",
      "LOSS train 0.003823164850473404 valid 0.04661739245057106, Kappa train 0.9586628790596124 valid 0.7854654480156148\n",
      "EPOCH 25:\n",
      "LOSS train 0.003204919630661607 valid 0.045114267617464066, Kappa train 0.9674226791968366 valid 0.7767772421686855\n",
      "EPOCH 26:\n",
      "LOSS train 0.002844687085598707 valid 0.04712986201047897, Kappa train 0.9653339090675943 valid 0.7736115432894102\n",
      "EPOCH 27:\n",
      "LOSS train 0.0026886314153671265 valid 0.04527080059051514, Kappa train 0.9667613038341177 valid 0.764476638259586\n",
      "EPOCH 28:\n",
      "LOSS train 0.002722755540162325 valid 0.04397723451256752, Kappa train 0.9689212512721893 valid 0.7828684108153297\n",
      "EPOCH 29:\n",
      "LOSS train 0.0024061461444944143 valid 0.043914180248975754, Kappa train 0.9701875325622649 valid 0.7795808506785721\n",
      "EPOCH 30:\n",
      "LOSS train 0.002399845514446497 valid 0.0455874465405941, Kappa train 0.9679004129177863 valid 0.7607379881985187\n",
      "EPOCH 31:\n",
      "LOSS train 0.0030489033088088036 valid 0.04803445562720299, Kappa train 0.9674206539224882 valid 0.7564079008991129\n",
      "EPOCH 32:\n",
      "LOSS train 0.008758842013776302 valid 0.0538112111389637, Kappa train 0.9282189874462364 valid 0.7323333419752031\n",
      "EPOCH 33:\n",
      "LOSS train 0.011845996603369713 valid 0.07223841547966003, Kappa train 0.9151394512716942 valid 0.6623866342897312\n",
      "EPOCH 34:\n",
      "LOSS train 0.011366085149347782 valid 0.048873696476221085, Kappa train 0.9200225433717365 valid 0.7436999934973342\n",
      "EPOCH 35:\n",
      "LOSS train 0.007945661433041096 valid 0.04981086403131485, Kappa train 0.9315580290176301 valid 0.7471954785827731\n",
      "EPOCH 36:\n",
      "LOSS train 0.005436863750219345 valid 0.048366013914346695, Kappa train 0.9444651159421036 valid 0.747932400770148\n",
      "EPOCH 37:\n",
      "LOSS train 0.005485926289111376 valid 0.049013398587703705, Kappa train 0.9424632573071087 valid 0.7524509283125961\n",
      "EPOCH 38:\n",
      "LOSS train 0.003453149227425456 valid 0.04571650177240372, Kappa train 0.9628673803327743 valid 0.7767280476771187\n",
      "EPOCH 39:\n",
      "LOSS train 0.002822360722348094 valid 0.04369443655014038, Kappa train 0.9678651091981088 valid 0.7608670925450547\n",
      "EPOCH 40:\n",
      "LOSS train 0.003213459625840187 valid 0.043999139219522476, Kappa train 0.966390626420085 valid 0.7775700998231001\n",
      "EPOCH 41:\n",
      "LOSS train 0.0028054912108927965 valid 0.0460708849132061, Kappa train 0.968383744595153 valid 0.7494350888777538\n",
      "EPOCH 42:\n",
      "LOSS train 0.0033629199024289846 valid 0.044495247304439545, Kappa train 0.9650363673596245 valid 0.7430726910745715\n",
      "EPOCH 43:\n",
      "LOSS train 0.005871130619198084 valid 0.04977835714817047, Kappa train 0.9489558026023062 valid 0.7580739835125289\n",
      "EPOCH 44:\n",
      "LOSS train 0.005663217045366764 valid 0.048580579459667206, Kappa train 0.9512344180709595 valid 0.7683365308802728\n",
      "EPOCH 45:\n",
      "LOSS train 0.0032887235283851624 valid 0.053105540573596954, Kappa train 0.9618192661812339 valid 0.7634934845064155\n",
      "EPOCH 46:\n",
      "LOSS train 0.0026442939415574074 valid 0.04299398139119148, Kappa train 0.9567150814286033 valid 0.782481315998522\n",
      "EPOCH 47:\n",
      "LOSS train 0.002464441116899252 valid 0.05005038529634476, Kappa train 0.9683107895379774 valid 0.7710842432768524\n",
      "EPOCH 48:\n",
      "LOSS train 0.02204234153032303 valid 0.04561074823141098, Kappa train 0.8610757535599901 valid 0.7704552662106968\n",
      "EPOCH 49:\n",
      "LOSS train 0.016220340505242348 valid 0.05288252979516983, Kappa train 0.8910196505217725 valid 0.7604706692006427\n",
      "EPOCH 50:\n",
      "LOSS train 0.00651973020285368 valid 0.04710138216614723, Kappa train 0.9410508947143794 valid 0.7775122433454964\n",
      "FOLD:  4\n",
      "EPOCH 1:\n",
      "LOSS train 0.08795535564422607 valid 0.21152979135513306, Kappa train 0.49851522652877467 valid 0.23308197393850838\n",
      "EPOCH 2:\n",
      "LOSS train 0.06567452847957611 valid 0.085036501288414, Kappa train 0.6470131890527628 valid 0.48519524588357066\n",
      "EPOCH 3:\n",
      "LOSS train 0.058577265590429306 valid 0.1166004166007042, Kappa train 0.6874323008690644 valid 0.3348393066400903\n",
      "EPOCH 4:\n",
      "LOSS train 0.04757661744952202 valid 0.11777487397193909, Kappa train 0.749191583361438 valid 0.37938913233556576\n",
      "EPOCH 5:\n",
      "LOSS train 0.043628908693790436 valid 0.05984777584671974, Kappa train 0.760917471868281 valid 0.6362649218310816\n",
      "EPOCH 6:\n",
      "LOSS train 0.03649482503533363 valid 0.06819604337215424, Kappa train 0.799239281756056 valid 0.6512124080320582\n",
      "EPOCH 7:\n",
      "LOSS train 0.03631381690502167 valid 0.07896887511014938, Kappa train 0.8018286699403737 valid 0.5747531116197653\n",
      "EPOCH 8:\n",
      "LOSS train 0.028137503191828728 valid 0.06580998748540878, Kappa train 0.8395445841017615 valid 0.6193654605110016\n",
      "EPOCH 9:\n",
      "LOSS train 0.02246749773621559 valid 0.07066599279642105, Kappa train 0.8675429144091557 valid 0.6133034808615367\n",
      "EPOCH 10:\n",
      "LOSS train 0.02107214741408825 valid 0.06440041959285736, Kappa train 0.8704155860668199 valid 0.5693501940507049\n",
      "EPOCH 11:\n",
      "LOSS train 0.014578290283679962 valid 0.05822265148162842, Kappa train 0.9021769310239084 valid 0.6545635662114584\n",
      "EPOCH 12:\n",
      "LOSS train 0.012336255982518196 valid 0.061996396631002426, Kappa train 0.9148648081555905 valid 0.6527867025917791\n",
      "EPOCH 13:\n",
      "LOSS train 0.013351773843169212 valid 0.06921793520450592, Kappa train 0.9092525754336214 valid 0.585515769153252\n",
      "EPOCH 14:\n",
      "LOSS train 0.011440136469900608 valid 0.0651644766330719, Kappa train 0.9191312728814789 valid 0.6488585223052251\n",
      "EPOCH 15:\n",
      "LOSS train 0.009688052348792553 valid 0.05632324516773224, Kappa train 0.9198072315799618 valid 0.6673482999690451\n",
      "EPOCH 16:\n",
      "LOSS train 0.007703985553234816 valid 0.05621390417218208, Kappa train 0.9373549755046028 valid 0.6616288565932463\n",
      "EPOCH 17:\n",
      "LOSS train 0.007356608286499977 valid 0.06339507550001144, Kappa train 0.938047870864969 valid 0.6294973952727565\n",
      "EPOCH 18:\n",
      "LOSS train 0.007492449134588242 valid 0.05463021993637085, Kappa train 0.9395472082451388 valid 0.6774575766622571\n",
      "EPOCH 19:\n",
      "LOSS train 0.0061727105639874935 valid 0.05068092420697212, Kappa train 0.9494847659347648 valid 0.6796656093588929\n",
      "EPOCH 20:\n",
      "LOSS train 0.005742765963077545 valid 0.051064036786556244, Kappa train 0.9469106237780591 valid 0.6917980875314184\n",
      "EPOCH 21:\n",
      "LOSS train 0.005318537820130587 valid 0.05486670881509781, Kappa train 0.9542063040412474 valid 0.6801807825289063\n",
      "EPOCH 22:\n",
      "LOSS train 0.004843309987336397 valid 0.051886048167943954, Kappa train 0.9587656766567114 valid 0.6775991393396936\n",
      "EPOCH 23:\n",
      "LOSS train 0.004603205714374781 valid 0.04812211915850639, Kappa train 0.9597205937395216 valid 0.6688881953763465\n",
      "EPOCH 24:\n",
      "LOSS train 0.005061011761426926 valid 0.057366687804460526, Kappa train 0.9556043596355139 valid 0.6483073661044599\n",
      "EPOCH 25:\n",
      "LOSS train 0.0050392732955515385 valid 0.055272020399570465, Kappa train 0.9547131389457808 valid 0.6484803143392298\n",
      "EPOCH 26:\n",
      "LOSS train 0.0043623074889183044 valid 0.052112508565187454, Kappa train 0.9605567100346286 valid 0.6811154357825766\n",
      "EPOCH 27:\n",
      "LOSS train 0.004431775771081448 valid 0.05313161015510559, Kappa train 0.9597906345911769 valid 0.6931782363974308\n",
      "EPOCH 28:\n",
      "LOSS train 0.004109261091798544 valid 0.051396843045949936, Kappa train 0.9612301095053288 valid 0.672170116495278\n",
      "EPOCH 29:\n",
      "LOSS train 0.00418673874810338 valid 0.050726454704999924, Kappa train 0.9605333430630374 valid 0.669411730655066\n",
      "EPOCH 30:\n",
      "LOSS train 0.004055221565067768 valid 0.05474960058927536, Kappa train 0.963010375424891 valid 0.6974895319410425\n",
      "EPOCH 31:\n",
      "LOSS train 0.005732199177145958 valid 0.0618373341858387, Kappa train 0.9542335336454296 valid 0.6678893676354415\n",
      "EPOCH 32:\n",
      "LOSS train 0.009911603294312954 valid 0.05484287813305855, Kappa train 0.9320291373577879 valid 0.678740749655756\n",
      "EPOCH 33:\n",
      "LOSS train 0.008358652703464031 valid 0.05864279344677925, Kappa train 0.9360463924998246 valid 0.6446502585691354\n",
      "EPOCH 34:\n",
      "LOSS train 0.00971075426787138 valid 0.056720055639743805, Kappa train 0.9323520858980184 valid 0.6351908265404792\n",
      "EPOCH 35:\n",
      "LOSS train 0.006818023975938559 valid 0.055022481828927994, Kappa train 0.9452107654538978 valid 0.6591556728674322\n",
      "EPOCH 36:\n",
      "LOSS train 0.005440892651677132 valid 0.056660573929548264, Kappa train 0.9541923696392299 valid 0.6709655265822572\n",
      "EPOCH 37:\n",
      "LOSS train 0.00401458190754056 valid 0.05259319022297859, Kappa train 0.9632137112107332 valid 0.6790414503042547\n",
      "EPOCH 38:\n",
      "LOSS train 0.0026714163832366467 valid 0.050816860049963, Kappa train 0.9713955510637214 valid 0.6813566934282568\n",
      "EPOCH 39:\n",
      "LOSS train 0.002141967648640275 valid 0.049903322011232376, Kappa train 0.9737896883068563 valid 0.7000072718803433\n",
      "EPOCH 40:\n",
      "LOSS train 0.0018691116711124778 valid 0.049170419573783875, Kappa train 0.978529831106136 valid 0.6950643967349347\n",
      "EPOCH 41:\n",
      "LOSS train 0.0018046064069494605 valid 0.04923909902572632, Kappa train 0.9786873786438607 valid 0.7150132455644908\n",
      "EPOCH 42:\n",
      "LOSS train 0.001720694126561284 valid 0.049132660031318665, Kappa train 0.9785429500927215 valid 0.6865817272991721\n",
      "EPOCH 43:\n",
      "LOSS train 0.0018106388160958886 valid 0.05054764077067375, Kappa train 0.9770625568311049 valid 0.6772109608273833\n",
      "EPOCH 44:\n",
      "LOSS train 0.001863398589193821 valid 0.048766374588012695, Kappa train 0.9782056598340061 valid 0.687300100673154\n",
      "EPOCH 45:\n",
      "LOSS train 0.0017068913439288735 valid 0.04830675199627876, Kappa train 0.980728850044896 valid 0.6929109723015354\n",
      "EPOCH 46:\n",
      "LOSS train 0.0015541829634457827 valid 0.050168588757514954, Kappa train 0.9836157912689819 valid 0.6581842739072384\n",
      "EPOCH 47:\n",
      "LOSS train 0.0015140293398872018 valid 0.04915323108434677, Kappa train 0.9817344211158822 valid 0.6990634154593426\n",
      "EPOCH 48:\n",
      "LOSS train 0.0015138820745050907 valid 0.04828937351703644, Kappa train 0.980543066982062 valid 0.7006423438594797\n",
      "EPOCH 49:\n",
      "LOSS train 0.0014443531399592757 valid 0.0487397275865078, Kappa train 0.979493008990718 valid 0.7022161690926427\n",
      "EPOCH 50:\n",
      "LOSS train 0.001417564577423036 valid 0.04855133965611458, Kappa train 0.9831864156944895 valid 0.6808934496746274\n",
      "FOLD:  5\n",
      "EPOCH 1:\n",
      "LOSS train 0.08718935400247574 valid 0.1668357253074646, Kappa train 0.4939159740181598 valid 0.27223688977904903\n",
      "EPOCH 2:\n",
      "LOSS train 0.060239024460315704 valid 0.06426836550235748, Kappa train 0.6692726425573035 valid 0.5843028028261679\n",
      "EPOCH 3:\n",
      "LOSS train 0.049595557153224945 valid 0.07126013189554214, Kappa train 0.7252197312039691 valid 0.5897582071971905\n",
      "EPOCH 4:\n",
      "LOSS train 0.040373411029577255 valid 0.06329646706581116, Kappa train 0.7820241252698017 valid 0.6229151850837473\n",
      "EPOCH 5:\n",
      "LOSS train 0.03472436964511871 valid 0.05762413144111633, Kappa train 0.8062200290424398 valid 0.6732435679016663\n",
      "EPOCH 6:\n",
      "LOSS train 0.0314200334250927 valid 0.08257820457220078, Kappa train 0.820681561864848 valid 0.6076204781026832\n",
      "EPOCH 7:\n",
      "LOSS train 0.0291930940002203 valid 0.07200109958648682, Kappa train 0.8317763703808542 valid 0.5873139867838192\n",
      "EPOCH 8:\n",
      "LOSS train 0.023123621940612793 valid 0.05732013285160065, Kappa train 0.8595704270660749 valid 0.6700876401221543\n",
      "EPOCH 9:\n",
      "LOSS train 0.016792122274637222 valid 0.05760171264410019, Kappa train 0.8868107787117732 valid 0.7229759666192815\n",
      "EPOCH 10:\n",
      "LOSS train 0.014064001850783825 valid 0.05176760256290436, Kappa train 0.9047386746918846 valid 0.7265964054547506\n",
      "EPOCH 11:\n",
      "LOSS train 0.01497174147516489 valid 0.052294664084911346, Kappa train 0.8923638306208055 valid 0.7072282356207218\n",
      "EPOCH 12:\n",
      "LOSS train 0.012804863974452019 valid 0.050557006150484085, Kappa train 0.9088407789235158 valid 0.719750178083361\n",
      "EPOCH 13:\n",
      "LOSS train 0.010587583296000957 valid 0.056987181305885315, Kappa train 0.9200670042394921 valid 0.726968659524107\n",
      "EPOCH 14:\n",
      "LOSS train 0.010365643538534641 valid 0.0544479563832283, Kappa train 0.9219217553912613 valid 0.7023456014482851\n",
      "EPOCH 15:\n",
      "LOSS train 0.008490013889968395 valid 0.06032559648156166, Kappa train 0.93398720413602 valid 0.674942435785614\n",
      "EPOCH 16:\n",
      "LOSS train 0.008205199614167213 valid 0.04766770079731941, Kappa train 0.9331952960946106 valid 0.7089019289440714\n",
      "EPOCH 17:\n",
      "LOSS train 0.005810976028442383 valid 0.045266300439834595, Kappa train 0.9501690315527913 valid 0.7505236342085642\n",
      "EPOCH 18:\n",
      "LOSS train 0.005366834346204996 valid 0.04646386206150055, Kappa train 0.9529659234487708 valid 0.7375331961122429\n",
      "EPOCH 19:\n",
      "LOSS train 0.0049288710579276085 valid 0.047902438789606094, Kappa train 0.9571202463428127 valid 0.7362736510037085\n",
      "EPOCH 20:\n",
      "LOSS train 0.004754881374537945 valid 0.0474216602742672, Kappa train 0.9570541724323232 valid 0.7271727591346013\n",
      "EPOCH 21:\n",
      "LOSS train 0.004573093261569738 valid 0.04619072750210762, Kappa train 0.9595254017158532 valid 0.7386434156555982\n",
      "EPOCH 22:\n",
      "LOSS train 0.004354126285761595 valid 0.047720469534397125, Kappa train 0.9635494174673637 valid 0.7452810775553925\n",
      "EPOCH 23:\n",
      "LOSS train 0.003932046238332987 valid 0.05058274790644646, Kappa train 0.9610901552447852 valid 0.7027814591774312\n",
      "EPOCH 24:\n",
      "LOSS train 0.0037925043143332005 valid 0.04527416080236435, Kappa train 0.965680118391774 valid 0.7554864908548478\n",
      "EPOCH 25:\n",
      "LOSS train 0.0034758637193590403 valid 0.053556933999061584, Kappa train 0.9693449438195902 valid 0.6865877597461717\n",
      "EPOCH 26:\n",
      "LOSS train 0.0047640553675591946 valid 0.04732620716094971, Kappa train 0.9606357935318214 valid 0.7164662080369736\n",
      "EPOCH 27:\n",
      "LOSS train 0.005148629657924175 valid 0.055325888097286224, Kappa train 0.9558355931915951 valid 0.7509886592603537\n",
      "EPOCH 28:\n",
      "LOSS train 0.006516897119581699 valid 0.0592203363776207, Kappa train 0.9500338220933653 valid 0.6842316846888117\n",
      "EPOCH 29:\n",
      "LOSS train 0.01414908841252327 valid 0.07272706180810928, Kappa train 0.9050948089135413 valid 0.6387407800331535\n",
      "EPOCH 30:\n",
      "LOSS train 0.017009884119033813 valid 0.058460138738155365, Kappa train 0.8866768472159997 valid 0.681267473648956\n",
      "EPOCH 31:\n",
      "LOSS train 0.012281405739486217 valid 0.05450921133160591, Kappa train 0.9135047912736173 valid 0.7049897751496614\n",
      "EPOCH 32:\n",
      "LOSS train 0.0069745490327477455 valid 0.046845726668834686, Kappa train 0.9453135851280786 valid 0.718854575658682\n",
      "EPOCH 33:\n",
      "LOSS train 0.004635611083358526 valid 0.05104103311896324, Kappa train 0.959571296313477 valid 0.7079077454108534\n",
      "EPOCH 34:\n",
      "LOSS train 0.0041210544295609 valid 0.04608451947569847, Kappa train 0.9605693977102776 valid 0.7550146826008114\n",
      "EPOCH 35:\n",
      "LOSS train 0.0035325742792338133 valid 0.049021683633327484, Kappa train 0.9656952802963712 valid 0.7269110083876584\n",
      "EPOCH 36:\n",
      "LOSS train 0.0029205125756561756 valid 0.05077455937862396, Kappa train 0.9704341486827512 valid 0.7092860616302847\n",
      "EPOCH 37:\n",
      "LOSS train 0.0026927245780825615 valid 0.046463742852211, Kappa train 0.9731243862176117 valid 0.74610085066143\n",
      "EPOCH 38:\n",
      "LOSS train 0.002066999673843384 valid 0.04569418355822563, Kappa train 0.976354525952911 valid 0.752915973192503\n",
      "EPOCH 39:\n",
      "LOSS train 0.001855232403613627 valid 0.046766821295022964, Kappa train 0.9777552255410223 valid 0.7283452786338482\n",
      "EPOCH 40:\n",
      "LOSS train 0.0018510391237214208 valid 0.04567578062415123, Kappa train 0.9782461797725598 valid 0.7470465351934832\n",
      "EPOCH 41:\n",
      "LOSS train 0.002128601772710681 valid 0.04803948104381561, Kappa train 0.9758686202557968 valid 0.7192082100615613\n",
      "EPOCH 42:\n",
      "LOSS train 0.0020339360926300287 valid 0.04684131219983101, Kappa train 0.9778068824744478 valid 0.7243673474777903\n",
      "EPOCH 43:\n",
      "LOSS train 0.001702548353932798 valid 0.0480673722922802, Kappa train 0.9808788415710256 valid 0.7380157361146665\n",
      "EPOCH 44:\n",
      "LOSS train 0.0027571276295930147 valid 0.06776641309261322, Kappa train 0.9738324231560409 valid 0.6000468374683487\n",
      "EPOCH 45:\n",
      "LOSS train 0.0035729024093598127 valid 0.0498555526137352, Kappa train 0.9652587253077057 valid 0.7176707970711748\n",
      "EPOCH 46:\n",
      "LOSS train 0.003204884473234415 valid 0.047280941158533096, Kappa train 0.9689876413708718 valid 0.7343665810734494\n",
      "EPOCH 47:\n",
      "LOSS train 0.0029766352381557226 valid 0.04885445162653923, Kappa train 0.9710482718770044 valid 0.7287797048094787\n",
      "EPOCH 48:\n",
      "LOSS train 0.002878604456782341 valid 0.04802070930600166, Kappa train 0.9690728924905103 valid 0.75444977211786\n",
      "EPOCH 49:\n",
      "LOSS train 0.0030343038961291313 valid 0.05021662265062332, Kappa train 0.9712795775994227 valid 0.7339309392732702\n",
      "EPOCH 50:\n",
      "LOSS train 0.002696809358894825 valid 0.04854453355073929, Kappa train 0.9728438250871141 valid 0.7386544226747727\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "folds = cross_fold.create_folded_datasets(\"../data/resized_224X224/label_info/labels.json\")\n",
    "\n",
    "root_dir = config['root_dir']\n",
    "\n",
    "annotations_file = config['annotations_path']\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if config['augment']:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomRotation(20),                           # Randomly rotate the image within -20 to +20 degrees\n",
    "        transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),# Randomly crop and resize the image to 224x224 pixels\n",
    "        transforms.RandomHorizontalFlip(0.2),                       # Randomly flip the image horizontally\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "else:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "val_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "saved_metrics = []\n",
    "\n",
    "for f_i,fold in enumerate(folds):\n",
    "    print(\"FOLD: \",f_i+1)\n",
    "    train_list, val_list = fold\n",
    "\n",
    "    model = create_model(config=config)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    train_dataset = midrc_challenge_dataset(root_dir, annotations_file, transform, fp_list = train_list)\n",
    "    val_dataset = midrc_challenge_dataset(root_dir, annotations_file, val_transform, fp_list = val_list)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size = config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = config['batch_size'], shuffle=True)\n",
    "\n",
    "    #Training per fold\n",
    "    metrics = train(config['epochs'],model,device,train_loader,val_loader, criterion, optimizer, f_i)\n",
    "    saved_metrics.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1  MAE: 0.14701972901821136  RMSE:  0.197721466422081  sMAPE:  0.6995524764060974  Kappa:  0.841319885147368\n",
      "Fold:  2  MAE: 0.15210509300231934  RMSE:  0.2021886706352234  sMAPE:  0.7592952847480774  Kappa:  0.734993897281661\n",
      "Fold:  3  MAE: 0.15506313741207123  RMSE:  0.2051122933626175  sMAPE:  0.7400298714637756  Kappa:  0.8203930942706129\n",
      "Fold:  4  MAE: 0.16975799202919006  RMSE:  0.21931613981723785  sMAPE:  0.7490667104721069  Kappa:  0.7179738680295065\n",
      "Fold:  5  MAE: 0.15526710450649261  RMSE:  0.21132703125476837  sMAPE:  0.8587353825569153  Kappa:  0.7897427968183617\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Testing Loop\n",
    "for f_i,fold in enumerate(folds):\n",
    "\n",
    "    train_list, val_list = fold\n",
    "\n",
    "    val_dataset = midrc_challenge_dataset(root_dir, annotations_file, val_transform, fp_list = val_list)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = 1, shuffle=True)\n",
    "\n",
    "    model = create_model(config)\n",
    "    model_pth = os.path.join('models','modelsave_fold_{}'.format(f_i))\n",
    "    model.load_state_dict(torch.load(model_pth))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    avg_mae, avg_rmse, avg_smape, kappa = test(model,device,val_loader)\n",
    "\n",
    "    print(\"Fold: \",f_i+1, \" MAE:\", avg_mae.item() , \" RMSE: \", avg_rmse.item(), \" sMAPE: \", avg_smape.item(), \" Kappa: \", kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'folds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfor\u001b[39;00m f_i,fold \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(folds):\n\u001b[0;32m      5\u001b[0m     fold_metrics \u001b[39m=\u001b[39m saved_metrics[f_i]\n\u001b[0;32m      7\u001b[0m     plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m12\u001b[39m,\u001b[39m8\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'folds' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "for f_i,fold in enumerate(folds):\n",
    "    fold_metrics = saved_metrics[f_i]\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.title(\"Losses for Fold: \"+ str(f_i+1))\n",
    "    plt.plot([fold_metrics[epoch]['avg_loss'] for epoch in fold_metrics], label='train_loss')\n",
    "    plt.plot([fold_metrics[epoch]['avg_vloss'] for epoch in fold_metrics],label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    filename = os.path.join(\"plots\",str(f_i+1)+'_losses.png')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.title(\"Kappas for Fold: \"+ str(f_i+1))\n",
    "    plt.plot([fold_metrics[epoch]['avg_train_kappa'] for epoch in fold_metrics], label='train_kappa')\n",
    "    plt.plot([fold_metrics[epoch]['avg_val_kappa'] for epoch in fold_metrics],label='val_kappa')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    filename = os.path.join(\"plots\",str(f_i+1)+'_kappas.png')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrale",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
