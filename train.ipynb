{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import cross_fold\n",
    "from midrc_dataset import midrc_challenge_dataset\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# ResNet 50 model \n",
    "def create_model():\n",
    "    model =  torchvision.models.resnet50()\n",
    "    model.conv1 = torch.nn.Conv2d(1,64, kernel_size=(7,7),stride=(2,2),padding=(3,3), bias=False)\n",
    "    model.fc = torch.nn.Sequential(torch.nn.Linear(2048, 1, bias=True), torch.nn.Sigmoid())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs,model,device, train_loader, val_loader, criterion, optimizer, fold_number):\n",
    "    best_vloss = 1_000_000.\n",
    "    for e in range(epochs):\n",
    "        print('EPOCH {}:'.format(e + 1))\n",
    "        running_loss = 0.\n",
    "        last_loss = 0.\n",
    "        model.train(True)\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            # Every data instance is an input + label pair\n",
    "            inputs, labels = data\n",
    "\n",
    "            # Zero your gradients for every batch!\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Make predictions for this batch\n",
    "            outputs = model(inputs.to(device))\n",
    "\n",
    "            # Compute the loss and its gradients\n",
    "            loss = criterion(outputs*torch.tensor(24.0).to(device), labels.float().to(device).unsqueeze(1))\n",
    "            loss.backward()\n",
    "\n",
    "            # Adjust learning weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Gather data and report\n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 9:\n",
    "                last_loss = running_loss / 10 # loss per batch\n",
    "                print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "                tb_x = e * len(train_loader) + i + 1\n",
    "                #tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "                running_loss = 0.\n",
    "\n",
    "        avg_loss = last_loss\n",
    "\n",
    "        running_vloss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(val_loader):\n",
    "                vinputs, vlabels = vdata\n",
    "                voutputs = model(vinputs.to(device))\n",
    "                vloss = criterion(voutputs*torch.tensor(24.0).to(device), vlabels.float().to(device).unsqueeze(1))\n",
    "                running_vloss += vloss\n",
    "\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "        # Track best performance, and save the model's state\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = os.path.join('models','modelsave_fold_{}'.format(fold_number))\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,device,dataloader):\n",
    "    with torch.no_grad():\n",
    "        squared_error = 0\n",
    "        absolute_error = 0\n",
    "\n",
    "        for data in dataloader:\n",
    "            inputs, labels = data\n",
    "\n",
    "            outputs = model(inputs.to(device))\n",
    "\n",
    "            squared_error += (outputs*torch.tensor(24.0).to(device) - labels.float().to(device).unsqueeze(1))**2\n",
    "            absolute_error += torch.abs(outputs*torch.tensor(24.0).to(device)-labels.float().to(device).unsqueeze(1))\n",
    "\n",
    "        #returns mae and rmse\n",
    "        return absolute_error/len(dataloader), torch.sqrt(squared_error/len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1  Train Indices:  2078  Val indices:  520\n",
      "Fold:  2  Train Indices:  2096  Val indices:  502\n",
      "Fold:  3  Train Indices:  2050  Val indices:  548\n",
      "Fold:  4  Train Indices:  2072  Val indices:  526\n",
      "Fold:  5  Train Indices:  2096  Val indices:  502\n",
      "EPOCH 1:\n",
      "  batch 10 loss: 96.03498115539551\n",
      "  batch 20 loss: 89.96414623260497\n",
      "  batch 30 loss: 79.36905670166016\n",
      "  batch 40 loss: 75.71682624816894\n",
      "  batch 50 loss: 73.0122745513916\n",
      "  batch 60 loss: 72.62853469848633\n",
      "  batch 70 loss: 86.35359954833984\n",
      "  batch 80 loss: 75.65400733947754\n",
      "  batch 90 loss: 80.2135612487793\n",
      "  batch 100 loss: 66.25345649719239\n",
      "  batch 110 loss: 59.41158485412598\n",
      "  batch 120 loss: 79.11459732055664\n",
      "  batch 130 loss: 60.30710868835449\n",
      "LOSS train 60.30710868835449 valid 117.91435241699219\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 73.36745491027833\n",
      "  batch 20 loss: 61.450362014770505\n",
      "  batch 30 loss: 74.70352096557617\n",
      "  batch 40 loss: 81.2291805267334\n",
      "  batch 50 loss: 86.8306957244873\n",
      "  batch 60 loss: 66.45353317260742\n",
      "  batch 70 loss: 81.30403594970703\n",
      "  batch 80 loss: 54.956722259521484\n",
      "  batch 90 loss: 57.29128227233887\n",
      "  batch 100 loss: 63.157616806030276\n",
      "  batch 110 loss: 65.17408638000488\n",
      "  batch 120 loss: 61.82555160522461\n",
      "  batch 130 loss: 64.48056297302246\n",
      "LOSS train 64.48056297302246 valid 66.4917984008789\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 63.254380798339845\n",
      "  batch 20 loss: 76.73274078369141\n",
      "  batch 30 loss: 59.47549591064453\n",
      "  batch 40 loss: 72.15267791748047\n",
      "  batch 50 loss: 69.91294746398925\n",
      "  batch 60 loss: 49.64180030822754\n",
      "  batch 70 loss: 62.070631408691405\n",
      "  batch 80 loss: 59.27683067321777\n",
      "  batch 90 loss: 53.6148567199707\n",
      "  batch 100 loss: 42.12282466888428\n",
      "  batch 110 loss: 49.49795684814453\n",
      "  batch 120 loss: 59.63840789794922\n",
      "  batch 130 loss: 44.29665298461914\n",
      "LOSS train 44.29665298461914 valid 76.23072814941406\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 52.74520034790039\n",
      "  batch 20 loss: 48.17851524353027\n",
      "  batch 30 loss: 46.174347496032716\n",
      "  batch 40 loss: 72.6130527496338\n",
      "  batch 50 loss: 91.05733604431153\n",
      "  batch 60 loss: 89.0315315246582\n",
      "  batch 70 loss: 61.649050903320315\n",
      "  batch 80 loss: 66.49443550109864\n",
      "  batch 90 loss: 60.639076232910156\n",
      "  batch 100 loss: 71.58957023620606\n",
      "  batch 110 loss: 51.31678771972656\n",
      "  batch 120 loss: 55.63572044372559\n",
      "  batch 130 loss: 50.61623344421387\n",
      "LOSS train 50.61623344421387 valid 58.719268798828125\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 56.98584327697754\n",
      "  batch 20 loss: 57.54060249328613\n",
      "  batch 30 loss: 42.96401119232178\n",
      "  batch 40 loss: 59.90549087524414\n",
      "  batch 50 loss: 57.20456428527832\n",
      "  batch 60 loss: 60.46078186035156\n",
      "  batch 70 loss: 55.4725284576416\n",
      "  batch 80 loss: 43.95256996154785\n",
      "  batch 90 loss: 48.97423057556152\n",
      "  batch 100 loss: 56.622137451171874\n",
      "  batch 110 loss: 54.17805557250976\n",
      "  batch 120 loss: 52.42627487182617\n",
      "  batch 130 loss: 50.499905014038085\n",
      "LOSS train 50.499905014038085 valid 69.51065826416016\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 47.73730659484863\n",
      "  batch 20 loss: 56.2217658996582\n",
      "  batch 30 loss: 45.58527326583862\n",
      "  batch 40 loss: 45.599134063720705\n",
      "  batch 50 loss: 53.799767303466794\n",
      "  batch 60 loss: 50.050611686706546\n",
      "  batch 70 loss: 40.76886882781982\n",
      "  batch 80 loss: 52.12353115081787\n",
      "  batch 90 loss: 46.80372009277344\n",
      "  batch 100 loss: 48.7300329208374\n",
      "  batch 110 loss: 47.45170955657959\n",
      "  batch 120 loss: 40.0591480255127\n",
      "  batch 130 loss: 47.03725700378418\n",
      "LOSS train 47.03725700378418 valid 45.06706619262695\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 45.54047031402588\n",
      "  batch 20 loss: 37.8957332611084\n",
      "  batch 30 loss: 36.61176643371582\n",
      "  batch 40 loss: 47.56152114868164\n",
      "  batch 50 loss: 54.21341762542725\n",
      "  batch 60 loss: 41.64063625335693\n",
      "  batch 70 loss: 44.399648666381836\n",
      "  batch 80 loss: 39.78030872344971\n",
      "  batch 90 loss: 43.55912857055664\n",
      "  batch 100 loss: 42.2529260635376\n",
      "  batch 110 loss: 42.9783842086792\n",
      "  batch 120 loss: 45.790527534484866\n",
      "  batch 130 loss: 42.308006858825685\n",
      "LOSS train 42.308006858825685 valid 46.056785583496094\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 36.51638317108154\n",
      "  batch 20 loss: 39.70459632873535\n",
      "  batch 30 loss: 44.790056991577146\n",
      "  batch 40 loss: 46.46952514648437\n",
      "  batch 50 loss: 44.65977039337158\n",
      "  batch 60 loss: 40.47270565032959\n",
      "  batch 70 loss: 39.84012985229492\n",
      "  batch 80 loss: 34.87194356918335\n",
      "  batch 90 loss: 40.30779552459717\n",
      "  batch 100 loss: 39.70758895874023\n",
      "  batch 110 loss: 40.65120620727539\n",
      "  batch 120 loss: 43.62892093658447\n",
      "  batch 130 loss: 37.987213897705075\n",
      "LOSS train 37.987213897705075 valid 37.489742279052734\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 39.01984786987305\n",
      "  batch 20 loss: 39.76788120269775\n",
      "  batch 30 loss: 37.98744411468506\n",
      "  batch 40 loss: 39.54168834686279\n",
      "  batch 50 loss: 40.161380195617674\n",
      "  batch 60 loss: 47.064936828613284\n",
      "  batch 70 loss: 39.680380249023436\n",
      "  batch 80 loss: 38.05054130554199\n",
      "  batch 90 loss: 35.21549339294434\n",
      "  batch 100 loss: 42.769015884399415\n",
      "  batch 110 loss: 45.740943908691406\n",
      "  batch 120 loss: 37.4905345916748\n",
      "  batch 130 loss: 31.41394672393799\n",
      "LOSS train 31.41394672393799 valid 63.51676559448242\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 40.02328758239746\n",
      "  batch 20 loss: 39.470347785949706\n",
      "  batch 30 loss: 50.10376396179199\n",
      "  batch 40 loss: 39.491156005859374\n",
      "  batch 50 loss: 35.66706256866455\n",
      "  batch 60 loss: 38.858142948150636\n",
      "  batch 70 loss: 32.16340045928955\n",
      "  batch 80 loss: 39.89703731536865\n",
      "  batch 90 loss: 38.4074426651001\n",
      "  batch 100 loss: 41.66191234588623\n",
      "  batch 110 loss: 40.170165634155275\n",
      "  batch 120 loss: 30.1939395904541\n",
      "  batch 130 loss: 28.940494441986083\n",
      "LOSS train 28.940494441986083 valid 38.33476638793945\n",
      "EPOCH 1:\n",
      "  batch 10 loss: 146.0413833618164\n",
      "  batch 20 loss: 125.9081901550293\n",
      "  batch 30 loss: 95.09387893676758\n",
      "  batch 40 loss: 74.5181785583496\n",
      "  batch 50 loss: 99.7849609375\n",
      "  batch 60 loss: 91.16131362915038\n",
      "  batch 70 loss: 81.96841011047363\n",
      "  batch 80 loss: 66.25984725952148\n",
      "  batch 90 loss: 74.32195167541504\n",
      "  batch 100 loss: 68.90701828002929\n",
      "  batch 110 loss: 85.22534942626953\n",
      "  batch 120 loss: 70.33834609985351\n",
      "  batch 130 loss: 90.87220306396485\n",
      "LOSS train 90.87220306396485 valid 57.981746673583984\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 85.74228286743164\n",
      "  batch 20 loss: 77.24970054626465\n",
      "  batch 30 loss: 67.49151649475098\n",
      "  batch 40 loss: 65.80685272216797\n",
      "  batch 50 loss: 69.35095672607422\n",
      "  batch 60 loss: 82.2060001373291\n",
      "  batch 70 loss: 72.5338363647461\n",
      "  batch 80 loss: 67.30315170288085\n",
      "  batch 90 loss: 65.58636703491212\n",
      "  batch 100 loss: 87.7911979675293\n",
      "  batch 110 loss: 64.61712303161622\n",
      "  batch 120 loss: 90.98352890014648\n",
      "  batch 130 loss: 78.05817680358886\n",
      "LOSS train 78.05817680358886 valid 82.86119842529297\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 59.50750885009766\n",
      "  batch 20 loss: 68.27007293701172\n",
      "  batch 30 loss: 63.99732208251953\n",
      "  batch 40 loss: 62.03774566650391\n",
      "  batch 50 loss: 67.57663383483887\n",
      "  batch 60 loss: 87.4380916595459\n",
      "  batch 70 loss: 58.34603786468506\n",
      "  batch 80 loss: 58.528828620910645\n",
      "  batch 90 loss: 67.43134994506836\n",
      "  batch 100 loss: 52.2757022857666\n",
      "  batch 110 loss: 65.26557006835938\n",
      "  batch 120 loss: 54.1355260848999\n",
      "  batch 130 loss: 56.44435501098633\n",
      "LOSS train 56.44435501098633 valid 51.223148345947266\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 43.459931182861325\n",
      "  batch 20 loss: 49.207832717895506\n",
      "  batch 30 loss: 51.035124969482425\n",
      "  batch 40 loss: 62.05542049407959\n",
      "  batch 50 loss: 50.689904880523684\n",
      "  batch 60 loss: 65.66891937255859\n",
      "  batch 70 loss: 74.10873718261719\n",
      "  batch 80 loss: 59.33408126831055\n",
      "  batch 90 loss: 58.36863746643066\n",
      "  batch 100 loss: 70.68881702423096\n",
      "  batch 110 loss: 61.54861488342285\n",
      "  batch 120 loss: 70.09714202880859\n",
      "  batch 130 loss: 69.78513660430909\n",
      "LOSS train 69.78513660430909 valid 47.7363166809082\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 49.87016716003418\n",
      "  batch 20 loss: 71.48623447418213\n",
      "  batch 30 loss: 69.39576148986816\n",
      "  batch 40 loss: 74.26942672729493\n",
      "  batch 50 loss: 80.69694900512695\n",
      "  batch 60 loss: 75.99565811157227\n",
      "  batch 70 loss: 74.3622657775879\n",
      "  batch 80 loss: 74.17523784637451\n",
      "  batch 90 loss: 65.99864234924317\n",
      "  batch 100 loss: 80.45803337097168\n",
      "  batch 110 loss: 71.16231575012208\n",
      "  batch 120 loss: 67.82682571411132\n",
      "  batch 130 loss: 76.82526359558105\n",
      "LOSS train 76.82526359558105 valid 61.0280647277832\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 65.42728652954102\n",
      "  batch 20 loss: 74.11274070739746\n",
      "  batch 30 loss: 81.11647415161133\n",
      "  batch 40 loss: 75.85766258239747\n",
      "  batch 50 loss: 59.42407455444336\n",
      "  batch 60 loss: 67.14275608062744\n",
      "  batch 70 loss: 57.661361122131346\n",
      "  batch 80 loss: 59.21885261535645\n",
      "  batch 90 loss: 57.864385986328124\n",
      "  batch 100 loss: 78.62737350463867\n",
      "  batch 110 loss: 54.25854225158692\n",
      "  batch 120 loss: 62.50779037475586\n",
      "  batch 130 loss: 56.26917018890381\n",
      "LOSS train 56.26917018890381 valid 72.25187683105469\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 52.45583019256592\n",
      "  batch 20 loss: 55.233584785461424\n",
      "  batch 30 loss: 52.07317352294922\n",
      "  batch 40 loss: 56.87283153533936\n",
      "  batch 50 loss: 52.88833656311035\n",
      "  batch 60 loss: 64.70137062072754\n",
      "  batch 70 loss: 50.96043739318848\n",
      "  batch 80 loss: 64.0732349395752\n",
      "  batch 90 loss: 64.24841918945313\n",
      "  batch 100 loss: 61.4085147857666\n",
      "  batch 110 loss: 55.77110633850098\n",
      "  batch 120 loss: 58.96849937438965\n",
      "  batch 130 loss: 57.03310928344727\n",
      "LOSS train 57.03310928344727 valid 43.12692642211914\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 60.84268455505371\n",
      "  batch 20 loss: 59.71801567077637\n",
      "  batch 30 loss: 54.32643013000488\n",
      "  batch 40 loss: 60.967655944824216\n",
      "  batch 50 loss: 52.66837768554687\n",
      "  batch 60 loss: 56.898705101013185\n",
      "  batch 70 loss: 48.3835147857666\n",
      "  batch 80 loss: 46.212684059143065\n",
      "  batch 90 loss: 47.45207118988037\n",
      "  batch 100 loss: 53.75608596801758\n",
      "  batch 110 loss: 55.940114212036136\n",
      "  batch 120 loss: 42.69284191131592\n",
      "  batch 130 loss: 48.34617366790771\n",
      "LOSS train 48.34617366790771 valid 81.63483428955078\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 58.82043170928955\n",
      "  batch 20 loss: 55.845407485961914\n",
      "  batch 30 loss: 52.79150047302246\n",
      "  batch 40 loss: 48.72703552246094\n",
      "  batch 50 loss: 44.37240467071533\n",
      "  batch 60 loss: 49.24847640991211\n",
      "  batch 70 loss: 46.318383598327635\n",
      "  batch 80 loss: 40.11499214172363\n",
      "  batch 90 loss: 54.47030181884766\n",
      "  batch 100 loss: 38.577237129211426\n",
      "  batch 110 loss: 40.6148078918457\n",
      "  batch 120 loss: 45.834801483154294\n",
      "  batch 130 loss: 37.29243812561035\n",
      "LOSS train 37.29243812561035 valid 58.46961212158203\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 46.16213207244873\n",
      "  batch 20 loss: 58.12874050140381\n",
      "  batch 30 loss: 49.24168167114258\n",
      "  batch 40 loss: 42.71289749145508\n",
      "  batch 50 loss: 41.92283325195312\n",
      "  batch 60 loss: 45.333182907104494\n",
      "  batch 70 loss: 35.36727066040039\n",
      "  batch 80 loss: 39.39531936645508\n",
      "  batch 90 loss: 36.24127330780029\n",
      "  batch 100 loss: 40.011060523986814\n",
      "  batch 110 loss: 39.26423225402832\n",
      "  batch 120 loss: 48.778055381774905\n",
      "  batch 130 loss: 54.62120666503906\n",
      "LOSS train 54.62120666503906 valid 64.42527770996094\n",
      "EPOCH 1:\n",
      "  batch 10 loss: 143.9608039855957\n",
      "  batch 20 loss: 109.58747825622558\n",
      "  batch 30 loss: 94.35250549316406\n",
      "  batch 40 loss: 82.48984413146972\n",
      "  batch 50 loss: 76.67242050170898\n",
      "  batch 60 loss: 83.79126739501953\n",
      "  batch 70 loss: 59.80752468109131\n",
      "  batch 80 loss: 77.77193660736084\n",
      "  batch 90 loss: 84.89115352630616\n",
      "  batch 100 loss: 79.09123382568359\n",
      "  batch 110 loss: 84.37668418884277\n",
      "  batch 120 loss: 107.31500434875488\n",
      "LOSS train 107.31500434875488 valid 159.03622436523438\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 91.92584571838378\n",
      "  batch 20 loss: 82.74183235168456\n",
      "  batch 30 loss: 79.31574020385742\n",
      "  batch 40 loss: 70.05853042602538\n",
      "  batch 50 loss: 68.78417587280273\n",
      "  batch 60 loss: 73.03995361328126\n",
      "  batch 70 loss: 76.24324760437011\n",
      "  batch 80 loss: 69.01598510742187\n",
      "  batch 90 loss: 52.13375816345215\n",
      "  batch 100 loss: 67.1241397857666\n",
      "  batch 110 loss: 65.09286231994629\n",
      "  batch 120 loss: 57.0815071105957\n",
      "LOSS train 57.0815071105957 valid 72.39799499511719\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 60.167962837219235\n",
      "  batch 20 loss: 56.981925582885744\n",
      "  batch 30 loss: 66.82420654296875\n",
      "  batch 40 loss: 78.46142501831055\n",
      "  batch 50 loss: 89.35225296020508\n",
      "  batch 60 loss: 78.33889503479004\n",
      "  batch 70 loss: 60.162506866455075\n",
      "  batch 80 loss: 56.71642665863037\n",
      "  batch 90 loss: 47.280155944824216\n",
      "  batch 100 loss: 59.208038902282716\n",
      "  batch 110 loss: 57.53710441589355\n",
      "  batch 120 loss: 50.15278778076172\n",
      "LOSS train 50.15278778076172 valid 85.87236022949219\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 58.02437629699707\n",
      "  batch 20 loss: 61.88491325378418\n",
      "  batch 30 loss: 58.08143196105957\n",
      "  batch 40 loss: 57.84842491149902\n",
      "  batch 50 loss: 49.49901103973389\n",
      "  batch 60 loss: 57.04567527770996\n",
      "  batch 70 loss: 59.048343658447266\n",
      "  batch 80 loss: 54.69975814819336\n",
      "  batch 90 loss: 50.79473247528076\n",
      "  batch 100 loss: 61.21167488098145\n",
      "  batch 110 loss: 57.34016742706299\n",
      "  batch 120 loss: 50.62054214477539\n",
      "LOSS train 50.62054214477539 valid 78.94374084472656\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 59.16819381713867\n",
      "  batch 20 loss: 55.21441459655762\n",
      "  batch 30 loss: 50.26440963745117\n",
      "  batch 40 loss: 68.02490501403808\n",
      "  batch 50 loss: 49.7721607208252\n",
      "  batch 60 loss: 39.477841186523435\n",
      "  batch 70 loss: 55.33931274414063\n",
      "  batch 80 loss: 67.38401107788086\n",
      "  batch 90 loss: 52.36433906555176\n",
      "  batch 100 loss: 64.2585220336914\n",
      "  batch 110 loss: 56.6299747467041\n",
      "  batch 120 loss: 52.950567436218265\n",
      "LOSS train 52.950567436218265 valid 86.55655670166016\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 51.18958206176758\n",
      "  batch 20 loss: 44.38200149536133\n",
      "  batch 30 loss: 53.44807739257813\n",
      "  batch 40 loss: 64.3631685256958\n",
      "  batch 50 loss: 57.19878978729248\n",
      "  batch 60 loss: 47.69303646087646\n",
      "  batch 70 loss: 51.48096771240235\n",
      "  batch 80 loss: 47.23064193725586\n",
      "  batch 90 loss: 41.928995513916014\n",
      "  batch 100 loss: 40.05571346282959\n",
      "  batch 110 loss: 43.71931209564209\n",
      "  batch 120 loss: 44.15639934539795\n",
      "LOSS train 44.15639934539795 valid 50.8243522644043\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 52.13454246520996\n",
      "  batch 20 loss: 47.46542453765869\n",
      "  batch 30 loss: 50.537730598449706\n",
      "  batch 40 loss: 43.9852783203125\n",
      "  batch 50 loss: 53.10993003845215\n",
      "  batch 60 loss: 44.89067134857178\n",
      "  batch 70 loss: 48.76947154998779\n",
      "  batch 80 loss: 43.516588020324704\n",
      "  batch 90 loss: 43.82986602783203\n",
      "  batch 100 loss: 32.508501052856445\n",
      "  batch 110 loss: 45.66950454711914\n",
      "  batch 120 loss: 42.833624267578124\n",
      "LOSS train 42.833624267578124 valid 47.48271560668945\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 40.56009654998779\n",
      "  batch 20 loss: 40.765288734436034\n",
      "  batch 30 loss: 43.76054306030274\n",
      "  batch 40 loss: 36.36044158935547\n",
      "  batch 50 loss: 43.5253568649292\n",
      "  batch 60 loss: 44.161923789978026\n",
      "  batch 70 loss: 38.84994735717773\n",
      "  batch 80 loss: 45.029194831848145\n",
      "  batch 90 loss: 36.30657520294189\n",
      "  batch 100 loss: 41.605743408203125\n",
      "  batch 110 loss: 51.387474822998044\n",
      "  batch 120 loss: 38.77183799743652\n",
      "LOSS train 38.77183799743652 valid 44.95929718017578\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 41.04448070526123\n",
      "  batch 20 loss: 34.36353607177735\n",
      "  batch 30 loss: 43.09723815917969\n",
      "  batch 40 loss: 56.35330657958984\n",
      "  batch 50 loss: 49.355338668823244\n",
      "  batch 60 loss: 44.663614654541014\n",
      "  batch 70 loss: 44.189396667480466\n",
      "  batch 80 loss: 37.405289840698245\n",
      "  batch 90 loss: 36.144448566436765\n",
      "  batch 100 loss: 37.415159034729\n",
      "  batch 110 loss: 37.592325592041014\n",
      "  batch 120 loss: 39.904836082458495\n",
      "LOSS train 39.904836082458495 valid 91.30490112304688\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 34.852323341369626\n",
      "  batch 20 loss: 43.629095268249515\n",
      "  batch 30 loss: 35.31461362838745\n",
      "  batch 40 loss: 33.37875232696533\n",
      "  batch 50 loss: 41.91465482711792\n",
      "  batch 60 loss: 34.52393074035645\n",
      "  batch 70 loss: 40.8364782333374\n",
      "  batch 80 loss: 46.50950317382812\n",
      "  batch 90 loss: 35.83023242950439\n",
      "  batch 100 loss: 36.69316234588623\n",
      "  batch 110 loss: 39.85619277954102\n",
      "  batch 120 loss: 46.836937713623044\n",
      "LOSS train 46.836937713623044 valid 37.0041618347168\n",
      "EPOCH 1:\n",
      "  batch 10 loss: 153.47988014221193\n",
      "  batch 20 loss: 93.5189323425293\n",
      "  batch 30 loss: 98.65734100341797\n",
      "  batch 40 loss: 90.68603820800782\n",
      "  batch 50 loss: 76.20638160705566\n",
      "  batch 60 loss: 77.35577964782715\n",
      "  batch 70 loss: 67.77474708557129\n",
      "  batch 80 loss: 79.26620597839356\n",
      "  batch 90 loss: 66.58639850616456\n",
      "  batch 100 loss: 67.89078636169434\n",
      "  batch 110 loss: 71.45561447143555\n",
      "  batch 120 loss: 71.30023460388183\n",
      "  batch 130 loss: 74.46912460327148\n",
      "LOSS train 74.46912460327148 valid 117.44158935546875\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 87.0995246887207\n",
      "  batch 20 loss: 79.41951789855958\n",
      "  batch 30 loss: 87.03652687072754\n",
      "  batch 40 loss: 78.58244323730469\n",
      "  batch 50 loss: 70.45648155212402\n",
      "  batch 60 loss: 61.6623348236084\n",
      "  batch 70 loss: 58.883745193481445\n",
      "  batch 80 loss: 79.72353515625\n",
      "  batch 90 loss: 63.904175186157225\n",
      "  batch 100 loss: 66.14875221252441\n",
      "  batch 110 loss: 62.945417976379396\n",
      "  batch 120 loss: 61.45091590881348\n",
      "  batch 130 loss: 67.91555824279786\n",
      "LOSS train 67.91555824279786 valid 78.47752380371094\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 50.76533470153809\n",
      "  batch 20 loss: 47.704965591430664\n",
      "  batch 30 loss: 68.44307250976563\n",
      "  batch 40 loss: 57.22608947753906\n",
      "  batch 50 loss: 69.39524421691894\n",
      "  batch 60 loss: 53.516546630859374\n",
      "  batch 70 loss: 60.285830688476565\n",
      "  batch 80 loss: 52.608245086669925\n",
      "  batch 90 loss: 68.04964904785156\n",
      "  batch 100 loss: 68.26629867553712\n",
      "  batch 110 loss: 60.85847415924072\n",
      "  batch 120 loss: 51.06039943695068\n",
      "  batch 130 loss: 71.26912918090821\n",
      "LOSS train 71.26912918090821 valid 88.61505889892578\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 56.344255447387695\n",
      "  batch 20 loss: 55.8014404296875\n",
      "  batch 30 loss: 54.474646759033206\n",
      "  batch 40 loss: 55.66520004272461\n",
      "  batch 50 loss: 44.18970909118652\n",
      "  batch 60 loss: 48.29260425567627\n",
      "  batch 70 loss: 54.91099472045899\n",
      "  batch 80 loss: 47.904626083374026\n",
      "  batch 90 loss: 61.97031173706055\n",
      "  batch 100 loss: 48.68562316894531\n",
      "  batch 110 loss: 49.588798904418944\n",
      "  batch 120 loss: 47.60417098999024\n",
      "  batch 130 loss: 35.58748722076416\n",
      "LOSS train 35.58748722076416 valid 51.85274124145508\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 35.611294174194335\n",
      "  batch 20 loss: 46.58580703735352\n",
      "  batch 30 loss: 50.208121109008786\n",
      "  batch 40 loss: 49.96565284729004\n",
      "  batch 50 loss: 47.14923629760742\n",
      "  batch 60 loss: 53.028971099853514\n",
      "  batch 70 loss: 42.509222984313965\n",
      "  batch 80 loss: 45.43844375610352\n",
      "  batch 90 loss: 43.367737007141116\n",
      "  batch 100 loss: 39.6824275970459\n",
      "  batch 110 loss: 46.55595722198486\n",
      "  batch 120 loss: 40.190676021575925\n",
      "  batch 130 loss: 55.73091011047363\n",
      "LOSS train 55.73091011047363 valid 51.6838493347168\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 39.60795593261719\n",
      "  batch 20 loss: 42.20446147918701\n",
      "  batch 30 loss: 49.18394031524658\n",
      "  batch 40 loss: 48.63324451446533\n",
      "  batch 50 loss: 46.820257568359374\n",
      "  batch 60 loss: 43.39843692779541\n",
      "  batch 70 loss: 51.046562576293944\n",
      "  batch 80 loss: 48.52485990524292\n",
      "  batch 90 loss: 54.211588096618655\n",
      "  batch 100 loss: 42.61085395812988\n",
      "  batch 110 loss: 46.09133796691894\n",
      "  batch 120 loss: 44.12619857788086\n",
      "  batch 130 loss: 36.33222846984863\n",
      "LOSS train 36.33222846984863 valid 86.14900970458984\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 38.268832778930665\n",
      "  batch 20 loss: 40.02977123260498\n",
      "  batch 30 loss: 35.08155212402344\n",
      "  batch 40 loss: 42.540595817565915\n",
      "  batch 50 loss: 37.0705545425415\n",
      "  batch 60 loss: 58.321333694458005\n",
      "  batch 70 loss: 35.97627143859863\n",
      "  batch 80 loss: 37.43301372528076\n",
      "  batch 90 loss: 42.76142196655273\n",
      "  batch 100 loss: 48.306647872924806\n",
      "  batch 110 loss: 48.7739013671875\n",
      "  batch 120 loss: 38.25195713043213\n",
      "  batch 130 loss: 28.952503395080566\n",
      "LOSS train 28.952503395080566 valid 38.553321838378906\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 35.92962856292725\n",
      "  batch 20 loss: 37.462855911254884\n",
      "  batch 30 loss: 30.245917129516602\n",
      "  batch 40 loss: 33.26197681427002\n",
      "  batch 50 loss: 35.735982513427736\n",
      "  batch 60 loss: 39.792511558532716\n",
      "  batch 70 loss: 32.98206825256348\n",
      "  batch 80 loss: 40.36011428833008\n",
      "  batch 90 loss: 31.349567127227782\n",
      "  batch 100 loss: 46.79322204589844\n",
      "  batch 110 loss: 42.75860404968262\n",
      "  batch 120 loss: 35.933976554870604\n",
      "  batch 130 loss: 31.80071563720703\n",
      "LOSS train 31.80071563720703 valid 45.20764923095703\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 26.626818466186524\n",
      "  batch 20 loss: 31.201832008361816\n",
      "  batch 30 loss: 48.44471054077148\n",
      "  batch 40 loss: 41.065893936157224\n",
      "  batch 50 loss: 33.101881980895996\n",
      "  batch 60 loss: 41.461321640014646\n",
      "  batch 70 loss: 34.097850227355956\n",
      "  batch 80 loss: 36.487005615234374\n",
      "  batch 90 loss: 43.479319381713864\n",
      "  batch 100 loss: 36.80947742462158\n",
      "  batch 110 loss: 43.40413341522217\n",
      "  batch 120 loss: 34.066203117370605\n",
      "  batch 130 loss: 37.615189170837404\n",
      "LOSS train 37.615189170837404 valid 67.44792175292969\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 35.18108215332031\n",
      "  batch 20 loss: 34.19962615966797\n",
      "  batch 30 loss: 34.32946891784668\n",
      "  batch 40 loss: 32.2971752166748\n",
      "  batch 50 loss: 37.533028030395506\n",
      "  batch 60 loss: 34.11213827133179\n",
      "  batch 70 loss: 38.04478435516357\n",
      "  batch 80 loss: 32.46717529296875\n",
      "  batch 90 loss: 26.71015281677246\n",
      "  batch 100 loss: 43.09152708053589\n",
      "  batch 110 loss: 34.85273313522339\n",
      "  batch 120 loss: 40.72089767456055\n",
      "  batch 130 loss: 37.30060214996338\n",
      "LOSS train 37.30060214996338 valid 64.49797821044922\n",
      "EPOCH 1:\n",
      "  batch 10 loss: 134.05367050170898\n",
      "  batch 20 loss: 83.8191146850586\n",
      "  batch 30 loss: 79.29105033874512\n",
      "  batch 40 loss: 78.82849082946777\n",
      "  batch 50 loss: 78.1192527770996\n",
      "  batch 60 loss: 73.29536018371581\n",
      "  batch 70 loss: 73.75999717712402\n",
      "  batch 80 loss: 68.7280487060547\n",
      "  batch 90 loss: 72.23239936828614\n",
      "  batch 100 loss: 70.6943962097168\n",
      "  batch 110 loss: 75.78677406311036\n",
      "  batch 120 loss: 57.88548736572265\n",
      "  batch 130 loss: 66.5920352935791\n",
      "LOSS train 66.5920352935791 valid 78.95502471923828\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 67.94527854919434\n",
      "  batch 20 loss: 55.14019203186035\n",
      "  batch 30 loss: 52.96397037506104\n",
      "  batch 40 loss: 68.72764663696289\n",
      "  batch 50 loss: 63.2535343170166\n",
      "  batch 60 loss: 54.665232276916505\n",
      "  batch 70 loss: 51.45153884887695\n",
      "  batch 80 loss: 48.76940650939942\n",
      "  batch 90 loss: 42.40653343200684\n",
      "  batch 100 loss: 62.457095336914065\n",
      "  batch 110 loss: 48.76543769836426\n",
      "  batch 120 loss: 48.20152950286865\n",
      "  batch 130 loss: 57.707096862792966\n",
      "LOSS train 57.707096862792966 valid 57.97563934326172\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 53.69581489562988\n",
      "  batch 20 loss: 58.77214736938477\n",
      "  batch 30 loss: 48.81285705566406\n",
      "  batch 40 loss: 46.867105674743655\n",
      "  batch 50 loss: 45.22671031951904\n",
      "  batch 60 loss: 50.791045379638675\n",
      "  batch 70 loss: 44.96419906616211\n",
      "  batch 80 loss: 46.25581588745117\n",
      "  batch 90 loss: 51.737705612182616\n",
      "  batch 100 loss: 60.286085510253905\n",
      "  batch 110 loss: 47.968752479553224\n",
      "  batch 120 loss: 51.53923416137695\n",
      "  batch 130 loss: 51.21768407821655\n",
      "LOSS train 51.21768407821655 valid 63.00410842895508\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 53.88727989196777\n",
      "  batch 20 loss: 57.27148361206055\n",
      "  batch 30 loss: 58.83924255371094\n",
      "  batch 40 loss: 47.1750207901001\n",
      "  batch 50 loss: 47.23965282440186\n",
      "  batch 60 loss: 41.6465202331543\n",
      "  batch 70 loss: 41.930188941955564\n",
      "  batch 80 loss: 53.39885692596435\n",
      "  batch 90 loss: 44.50441055297851\n",
      "  batch 100 loss: 55.308870506286624\n",
      "  batch 110 loss: 47.13202056884766\n",
      "  batch 120 loss: 33.123603630065915\n",
      "  batch 130 loss: 43.04995632171631\n",
      "LOSS train 43.04995632171631 valid 56.61356735229492\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 44.124939918518066\n",
      "  batch 20 loss: 40.6669261932373\n",
      "  batch 30 loss: 41.775047302246094\n",
      "  batch 40 loss: 50.69759960174561\n",
      "  batch 50 loss: 49.48296146392822\n",
      "  batch 60 loss: 52.38493041992187\n",
      "  batch 70 loss: 47.6778980255127\n",
      "  batch 80 loss: 42.12239284515381\n",
      "  batch 90 loss: 40.70768051147461\n",
      "  batch 100 loss: 39.98310775756836\n",
      "  batch 110 loss: 35.48162450790405\n",
      "  batch 120 loss: 38.25112609863281\n",
      "  batch 130 loss: 37.04523220062256\n",
      "LOSS train 37.04523220062256 valid 41.37648391723633\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 27.625597763061524\n",
      "  batch 20 loss: 37.54146308898926\n",
      "  batch 30 loss: 36.940739917755124\n",
      "  batch 40 loss: 51.97935905456543\n",
      "  batch 50 loss: 55.703949737548825\n",
      "  batch 60 loss: 47.717164039611816\n",
      "  batch 70 loss: 36.56962070465088\n",
      "  batch 80 loss: 41.70110397338867\n",
      "  batch 90 loss: 49.18600311279297\n",
      "  batch 100 loss: 32.09239025115967\n",
      "  batch 110 loss: 45.88698272705078\n",
      "  batch 120 loss: 40.225161743164065\n",
      "  batch 130 loss: 27.301533889770507\n",
      "LOSS train 27.301533889770507 valid 84.33672332763672\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 37.180924797058104\n",
      "  batch 20 loss: 32.31168956756592\n",
      "  batch 30 loss: 41.16052932739258\n",
      "  batch 40 loss: 34.29461479187012\n",
      "  batch 50 loss: 39.764092445373535\n",
      "  batch 60 loss: 33.33609161376953\n",
      "  batch 70 loss: 37.46241474151611\n",
      "  batch 80 loss: 46.97110023498535\n",
      "  batch 90 loss: 39.54223079681397\n",
      "  batch 100 loss: 37.62078304290772\n",
      "  batch 110 loss: 36.392053031921385\n",
      "  batch 120 loss: 35.66380548477173\n",
      "  batch 130 loss: 41.31287326812744\n",
      "LOSS train 41.31287326812744 valid 51.60575866699219\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 40.233048629760745\n",
      "  batch 20 loss: 28.579285621643066\n",
      "  batch 30 loss: 42.42418594360352\n",
      "  batch 40 loss: 33.991235446929934\n",
      "  batch 50 loss: 36.26406307220459\n",
      "  batch 60 loss: 34.496100425720215\n",
      "  batch 70 loss: 43.288590621948245\n",
      "  batch 80 loss: 39.7850378036499\n",
      "  batch 90 loss: 30.91811933517456\n",
      "  batch 100 loss: 36.74294710159302\n",
      "  batch 110 loss: 35.88517608642578\n",
      "  batch 120 loss: 30.90628261566162\n",
      "  batch 130 loss: 38.65655727386475\n",
      "LOSS train 38.65655727386475 valid 46.249454498291016\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 36.62912254333496\n",
      "  batch 20 loss: 32.97457580566406\n",
      "  batch 30 loss: 35.11697978973389\n",
      "  batch 40 loss: 41.1511775970459\n",
      "  batch 50 loss: 31.306772422790527\n",
      "  batch 60 loss: 29.932114791870116\n",
      "  batch 70 loss: 33.964318084716794\n",
      "  batch 80 loss: 37.98588943481445\n",
      "  batch 90 loss: 31.86621685028076\n",
      "  batch 100 loss: 44.84316291809082\n",
      "  batch 110 loss: 45.49531993865967\n",
      "  batch 120 loss: 37.53373985290527\n",
      "  batch 130 loss: 37.52746181488037\n",
      "LOSS train 37.52746181488037 valid 48.077327728271484\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 34.32258853912354\n",
      "  batch 20 loss: 27.445105934143065\n",
      "  batch 30 loss: 43.61169586181641\n",
      "  batch 40 loss: 28.509849548339844\n",
      "  batch 50 loss: 40.03613615036011\n",
      "  batch 60 loss: 37.1020586013794\n",
      "  batch 70 loss: 29.350114822387695\n",
      "  batch 80 loss: 36.3638557434082\n",
      "  batch 90 loss: 28.576825046539305\n",
      "  batch 100 loss: 34.310288429260254\n",
      "  batch 110 loss: 34.59986839294434\n",
      "  batch 120 loss: 33.4581862449646\n",
      "  batch 130 loss: 33.633051776885985\n",
      "LOSS train 33.633051776885985 valid 38.950687408447266\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "folds = cross_fold.create_folded_datasets(\"C:\\\\Users\\\\CGarc\\\\University of Kentucky\\\\Ahamed, Md. Atik - COVID severity\\\\data\\\\resized_224X224\\\\label_info\\\\labels.json\")\n",
    "\n",
    "root_dir = 'C:\\\\Users\\\\CGarc\\\\University of Kentucky\\\\Ahamed, Md. Atik - COVID severity\\\\data\\\\resized_224X224'\n",
    "\n",
    "annotations_file = 'MIDRC mRALE Mastermind Training Annotations_2079_20230428.csv'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 10\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    #    transforms.RandomHorizontalFlip(),\n",
    "    #    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "for f_i,fold in enumerate(folds):\n",
    "    train_list, val_list = fold\n",
    "\n",
    "    model = create_model()\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    train_dataset = midrc_challenge_dataset(root_dir, annotations_file, transform, fp_list = train_list)\n",
    "    val_dataset = midrc_challenge_dataset(root_dir, annotations_file, transform, fp_list = val_list)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "    #Training per fold\n",
    "    train(epochs,model,device,train_loader,val_loader, criterion, optimizer, f_i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1  MAE: 5.070911407470703  RMSE:  6.121790885925293\n",
      "Fold:  2  MAE: 5.229495525360107  RMSE:  6.513065814971924\n",
      "Fold:  3  MAE: 4.875302791595459  RMSE:  6.091127872467041\n",
      "Fold:  4  MAE: 5.047328472137451  RMSE:  6.208610534667969\n",
      "Fold:  5  MAE: 4.741214752197266  RMSE:  6.251406192779541\n"
     ]
    }
   ],
   "source": [
    "#Testing Loop\n",
    "for f_i,fold in enumerate(folds):\n",
    "\n",
    "    train_list, val_list = fold\n",
    "\n",
    "    val_dataset = midrc_challenge_dataset(root_dir, annotations_file, transform, fp_list = val_list)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = 1, shuffle=True)\n",
    "\n",
    "    model = create_model()\n",
    "    model_pth = os.path.join('models','modelsave_fold_{}'.format(f_i))\n",
    "    model.load_state_dict(torch.load(model_pth))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    avg_mae, avg_rmse = test(model,device,val_loader)\n",
    "\n",
    "    print(\"Fold: \",f_i+1, \" MAE:\", avg_mae.item() , \" RMSE: \", avg_rmse.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrale",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
